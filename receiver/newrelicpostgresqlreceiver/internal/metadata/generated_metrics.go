// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/filter"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

// AttributeOperation specifies the value operation attribute.
type AttributeOperation int

const (
	_ AttributeOperation = iota
	AttributeOperationIns
	AttributeOperationUpd
	AttributeOperationDel
	AttributeOperationHotUpd
)

// String returns the string representation of the AttributeOperation.
func (av AttributeOperation) String() string {
	switch av {
	case AttributeOperationIns:
		return "ins"
	case AttributeOperationUpd:
		return "upd"
	case AttributeOperationDel:
		return "del"
	case AttributeOperationHotUpd:
		return "hot_upd"
	}
	return ""
}

// MapAttributeOperation is a helper map of string to AttributeOperation attribute value.
var MapAttributeOperation = map[string]AttributeOperation{
	"ins":     AttributeOperationIns,
	"upd":     AttributeOperationUpd,
	"del":     AttributeOperationDel,
	"hot_upd": AttributeOperationHotUpd,
}

// AttributeSource specifies the value source attribute.
type AttributeSource int

const (
	_ AttributeSource = iota
	AttributeSourceHeapRead
	AttributeSourceHeapHit
	AttributeSourceIdxRead
	AttributeSourceIdxHit
	AttributeSourceToastRead
	AttributeSourceToastHit
	AttributeSourceTidxRead
	AttributeSourceTidxHit
)

// String returns the string representation of the AttributeSource.
func (av AttributeSource) String() string {
	switch av {
	case AttributeSourceHeapRead:
		return "heap_read"
	case AttributeSourceHeapHit:
		return "heap_hit"
	case AttributeSourceIdxRead:
		return "idx_read"
	case AttributeSourceIdxHit:
		return "idx_hit"
	case AttributeSourceToastRead:
		return "toast_read"
	case AttributeSourceToastHit:
		return "toast_hit"
	case AttributeSourceTidxRead:
		return "tidx_read"
	case AttributeSourceTidxHit:
		return "tidx_hit"
	}
	return ""
}

// MapAttributeSource is a helper map of string to AttributeSource attribute value.
var MapAttributeSource = map[string]AttributeSource{
	"heap_read":  AttributeSourceHeapRead,
	"heap_hit":   AttributeSourceHeapHit,
	"idx_read":   AttributeSourceIdxRead,
	"idx_hit":    AttributeSourceIdxHit,
	"toast_read": AttributeSourceToastRead,
	"toast_hit":  AttributeSourceToastHit,
	"tidx_read":  AttributeSourceTidxRead,
	"tidx_hit":   AttributeSourceTidxHit,
}

// AttributeState specifies the value state attribute.
type AttributeState int

const (
	_ AttributeState = iota
	AttributeStateDead
	AttributeStateLive
)

// String returns the string representation of the AttributeState.
func (av AttributeState) String() string {
	switch av {
	case AttributeStateDead:
		return "dead"
	case AttributeStateLive:
		return "live"
	}
	return ""
}

// MapAttributeState is a helper map of string to AttributeState attribute value.
var MapAttributeState = map[string]AttributeState{
	"dead": AttributeStateDead,
	"live": AttributeStateLive,
}

var MetricsInfo = metricsInfo{
	PostgresqlBlockedSessionPid: metricInfo{
		Name: "postgresql.blocked.session.pid",
	},
	PostgresqlBlockingSessionPid: metricInfo{
		Name: "postgresql.blocking.session.pid",
	},
	PostgresqlBlocksHit: metricInfo{
		Name: "postgresql.blocks_hit",
	},
	PostgresqlBlocksRead: metricInfo{
		Name: "postgresql.blocks_read",
	},
	PostgresqlCommits: metricInfo{
		Name: "postgresql.commits",
	},
	PostgresqlConnectionCount: metricInfo{
		Name: "postgresql.connection.count",
	},
	PostgresqlConnectionMax: metricInfo{
		Name: "postgresql.connection.max",
	},
	PostgresqlDatabaseCount: metricInfo{
		Name: "postgresql.database.count",
	},
	PostgresqlDatabaseLocks: metricInfo{
		Name: "postgresql.database.locks",
	},
	PostgresqlExecutionPlanActualLoops: metricInfo{
		Name: "postgresql.execution_plan.actual_loops",
	},
	PostgresqlExecutionPlanActualRows: metricInfo{
		Name: "postgresql.execution_plan.actual_rows",
	},
	PostgresqlExecutionPlanActualTotalTime: metricInfo{
		Name: "postgresql.execution_plan.actual_total_time",
	},
	PostgresqlExecutionPlanAsyncCapable: metricInfo{
		Name: "postgresql.execution_plan.async_capable",
	},
	PostgresqlExecutionPlanParallelAware: metricInfo{
		Name: "postgresql.execution_plan.parallel_aware",
	},
	PostgresqlIndexScans: metricInfo{
		Name: "postgresql.index.scans",
	},
	PostgresqlIndexSize: metricInfo{
		Name: "postgresql.index.size",
	},
	PostgresqlOperations: metricInfo{
		Name: "postgresql.operations",
	},
	PostgresqlQueryAvgDiskReads: metricInfo{
		Name: "postgresql.query.avg_disk_reads",
	},
	PostgresqlQueryAvgDiskWrites: metricInfo{
		Name: "postgresql.query.avg_disk_writes",
	},
	PostgresqlQueryAvgElapsedTime: metricInfo{
		Name: "postgresql.query.avg_elapsed_time",
	},
	PostgresqlQueryCPUTime: metricInfo{
		Name: "postgresql.query.cpu_time",
	},
	PostgresqlQueryExecutionCount: metricInfo{
		Name: "postgresql.query.execution.count",
	},
	PostgresqlRollbacks: metricInfo{
		Name: "postgresql.rollbacks",
	},
	PostgresqlRows: metricInfo{
		Name: "postgresql.rows",
	},
	PostgresqlTableScans: metricInfo{
		Name: "postgresql.table.scans",
	},
	PostgresqlTableSize: metricInfo{
		Name: "postgresql.table.size",
	},
	PostgresqlTableVacuumCount: metricInfo{
		Name: "postgresql.table.vacuum.count",
	},
	PostgresqlWaitEventTotalTime: metricInfo{
		Name: "postgresql.wait.event.total_time",
	},
	PostgresqlWalAge: metricInfo{
		Name: "postgresql.wal.age",
	},
	PostgresqlWalLag: metricInfo{
		Name: "postgresql.wal.lag",
	},
}

type metricsInfo struct {
	PostgresqlBlockedSessionPid            metricInfo
	PostgresqlBlockingSessionPid           metricInfo
	PostgresqlBlocksHit                    metricInfo
	PostgresqlBlocksRead                   metricInfo
	PostgresqlCommits                      metricInfo
	PostgresqlConnectionCount              metricInfo
	PostgresqlConnectionMax                metricInfo
	PostgresqlDatabaseCount                metricInfo
	PostgresqlDatabaseLocks                metricInfo
	PostgresqlExecutionPlanActualLoops     metricInfo
	PostgresqlExecutionPlanActualRows      metricInfo
	PostgresqlExecutionPlanActualTotalTime metricInfo
	PostgresqlExecutionPlanAsyncCapable    metricInfo
	PostgresqlExecutionPlanParallelAware   metricInfo
	PostgresqlIndexScans                   metricInfo
	PostgresqlIndexSize                    metricInfo
	PostgresqlOperations                   metricInfo
	PostgresqlQueryAvgDiskReads            metricInfo
	PostgresqlQueryAvgDiskWrites           metricInfo
	PostgresqlQueryAvgElapsedTime          metricInfo
	PostgresqlQueryCPUTime                 metricInfo
	PostgresqlQueryExecutionCount          metricInfo
	PostgresqlRollbacks                    metricInfo
	PostgresqlRows                         metricInfo
	PostgresqlTableScans                   metricInfo
	PostgresqlTableSize                    metricInfo
	PostgresqlTableVacuumCount             metricInfo
	PostgresqlWaitEventTotalTime           metricInfo
	PostgresqlWalAge                       metricInfo
	PostgresqlWalLag                       metricInfo
}

type metricInfo struct {
	Name string
}

type metricPostgresqlBlockedSessionPid struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocked.session.pid metric with initial data.
func (m *metricPostgresqlBlockedSessionPid) init() {
	m.data.SetName("postgresql.blocked.session.pid")
	m.data.SetDescription("Process ID of the blocked session.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlockedSessionPid) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockedSessionPid) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockedSessionPid) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockedSessionPid(cfg MetricConfig) metricPostgresqlBlockedSessionPid {
	m := metricPostgresqlBlockedSessionPid{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlockingSessionPid struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocking.session.pid metric with initial data.
func (m *metricPostgresqlBlockingSessionPid) init() {
	m.data.SetName("postgresql.blocking.session.pid")
	m.data.SetDescription("Process ID of the blocking session.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlockingSessionPid) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockingSessionPid) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockingSessionPid) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockingSessionPid(cfg MetricConfig) metricPostgresqlBlockingSessionPid {
	m := metricPostgresqlBlockingSessionPid{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlocksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocks_hit metric with initial data.
func (m *metricPostgresqlBlocksHit) init() {
	m.data.SetName("postgresql.blocks_hit")
	m.data.SetDescription("The number of blocks found in the buffer cache.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlocksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("source", sourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlocksHit) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlocksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlocksHit(cfg MetricConfig) metricPostgresqlBlocksHit {
	m := metricPostgresqlBlocksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlocksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocks_read metric with initial data.
func (m *metricPostgresqlBlocksRead) init() {
	m.data.SetName("postgresql.blocks_read")
	m.data.SetDescription("The number of blocks read.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlocksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("source", sourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlocksRead) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlocksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlocksRead(cfg MetricConfig) metricPostgresqlBlocksRead {
	m := metricPostgresqlBlocksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCommits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.commits metric with initial data.
func (m *metricPostgresqlCommits) init() {
	m.data.SetName("postgresql.commits")
	m.data.SetDescription("The number of commits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCommits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCommits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCommits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCommits(cfg MetricConfig) metricPostgresqlCommits {
	m := metricPostgresqlCommits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConnectionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.connection.count metric with initial data.
func (m *metricPostgresqlConnectionCount) init() {
	m.data.SetName("postgresql.connection.count")
	m.data.SetDescription("Number of user connections.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConnectionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConnectionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConnectionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConnectionCount(cfg MetricConfig) metricPostgresqlConnectionCount {
	m := metricPostgresqlConnectionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConnectionMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.connection.max metric with initial data.
func (m *metricPostgresqlConnectionMax) init() {
	m.data.SetName("postgresql.connection.max")
	m.data.SetDescription("Maximum number of client connections allowed.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricPostgresqlConnectionMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConnectionMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConnectionMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConnectionMax(cfg MetricConfig) metricPostgresqlConnectionMax {
	m := metricPostgresqlConnectionMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database.count metric with initial data.
func (m *metricPostgresqlDatabaseCount) init() {
	m.data.SetName("postgresql.database.count")
	m.data.SetDescription("Number of user databases.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
}

func (m *metricPostgresqlDatabaseCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseCount(cfg MetricConfig) metricPostgresqlDatabaseCount {
	m := metricPostgresqlDatabaseCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseLocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database.locks metric with initial data.
func (m *metricPostgresqlDatabaseLocks) init() {
	m.data.SetName("postgresql.database.locks")
	m.data.SetDescription("The number of database locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDatabaseLocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseLocks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseLocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseLocks(cfg MetricConfig) metricPostgresqlDatabaseLocks {
	m := metricPostgresqlDatabaseLocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanActualLoops struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.actual_loops metric with initial data.
func (m *metricPostgresqlExecutionPlanActualLoops) init() {
	m.data.SetName("postgresql.execution_plan.actual_loops")
	m.data.SetDescription("Number of times the execution plan node was executed.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanActualLoops) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanActualLoops) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanActualLoops) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanActualLoops(cfg MetricConfig) metricPostgresqlExecutionPlanActualLoops {
	m := metricPostgresqlExecutionPlanActualLoops{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanActualRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.actual_rows metric with initial data.
func (m *metricPostgresqlExecutionPlanActualRows) init() {
	m.data.SetName("postgresql.execution_plan.actual_rows")
	m.data.SetDescription("Actual number of rows processed by the execution plan node.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanActualRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanActualRows) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanActualRows) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanActualRows(cfg MetricConfig) metricPostgresqlExecutionPlanActualRows {
	m := metricPostgresqlExecutionPlanActualRows{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanActualTotalTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.actual_total_time metric with initial data.
func (m *metricPostgresqlExecutionPlanActualTotalTime) init() {
	m.data.SetName("postgresql.execution_plan.actual_total_time")
	m.data.SetDescription("Total time spent in the execution plan node in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanActualTotalTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanActualTotalTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanActualTotalTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanActualTotalTime(cfg MetricConfig) metricPostgresqlExecutionPlanActualTotalTime {
	m := metricPostgresqlExecutionPlanActualTotalTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanAsyncCapable struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.async_capable metric with initial data.
func (m *metricPostgresqlExecutionPlanAsyncCapable) init() {
	m.data.SetName("postgresql.execution_plan.async_capable")
	m.data.SetDescription("Whether the execution plan node is async capable.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanAsyncCapable) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanAsyncCapable) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanAsyncCapable) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanAsyncCapable(cfg MetricConfig) metricPostgresqlExecutionPlanAsyncCapable {
	m := metricPostgresqlExecutionPlanAsyncCapable{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanParallelAware struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.parallel_aware metric with initial data.
func (m *metricPostgresqlExecutionPlanParallelAware) init() {
	m.data.SetName("postgresql.execution_plan.parallel_aware")
	m.data.SetDescription("Whether the execution plan node is parallel aware.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanParallelAware) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanParallelAware) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanParallelAware) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanParallelAware(cfg MetricConfig) metricPostgresqlExecutionPlanParallelAware {
	m := metricPostgresqlExecutionPlanParallelAware{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexScans struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index.scans metric with initial data.
func (m *metricPostgresqlIndexScans) init() {
	m.data.SetName("postgresql.index.scans")
	m.data.SetDescription("The number of index scans on a table.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexScans) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("postgresql.index.name", postgresqlIndexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexScans) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexScans) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexScans(cfg MetricConfig) metricPostgresqlIndexScans {
	m := metricPostgresqlIndexScans{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index.size metric with initial data.
func (m *metricPostgresqlIndexSize) init() {
	m.data.SetName("postgresql.index.size")
	m.data.SetDescription("The size of the index on disk.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("postgresql.index.name", postgresqlIndexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexSize(cfg MetricConfig) metricPostgresqlIndexSize {
	m := metricPostgresqlIndexSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlOperations struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.operations metric with initial data.
func (m *metricPostgresqlOperations) init() {
	m.data.SetName("postgresql.operations")
	m.data.SetDescription("The number of database operations.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlOperations) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, operationAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("operation", operationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlOperations) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlOperations) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlOperations(cfg MetricConfig) metricPostgresqlOperations {
	m := metricPostgresqlOperations{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryAvgDiskReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.avg_disk_reads metric with initial data.
func (m *metricPostgresqlQueryAvgDiskReads) init() {
	m.data.SetName("postgresql.query.avg_disk_reads")
	m.data.SetDescription("Average number of disk reads per query execution.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryAvgDiskReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryAvgDiskReads) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryAvgDiskReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryAvgDiskReads(cfg MetricConfig) metricPostgresqlQueryAvgDiskReads {
	m := metricPostgresqlQueryAvgDiskReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryAvgDiskWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.avg_disk_writes metric with initial data.
func (m *metricPostgresqlQueryAvgDiskWrites) init() {
	m.data.SetName("postgresql.query.avg_disk_writes")
	m.data.SetDescription("Average number of disk writes per query execution.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryAvgDiskWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryAvgDiskWrites) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryAvgDiskWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryAvgDiskWrites(cfg MetricConfig) metricPostgresqlQueryAvgDiskWrites {
	m := metricPostgresqlQueryAvgDiskWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryAvgElapsedTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.avg_elapsed_time metric with initial data.
func (m *metricPostgresqlQueryAvgElapsedTime) init() {
	m.data.SetName("postgresql.query.avg_elapsed_time")
	m.data.SetDescription("Average execution time for the query in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryAvgElapsedTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryAvgElapsedTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryAvgElapsedTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryAvgElapsedTime(cfg MetricConfig) metricPostgresqlQueryAvgElapsedTime {
	m := metricPostgresqlQueryAvgElapsedTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryCPUTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.cpu_time metric with initial data.
func (m *metricPostgresqlQueryCPUTime) init() {
	m.data.SetName("postgresql.query.cpu_time")
	m.data.SetDescription("CPU time consumed by the query in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryCPUTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryCPUTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryCPUTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryCPUTime(cfg MetricConfig) metricPostgresqlQueryCPUTime {
	m := metricPostgresqlQueryCPUTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryExecutionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.execution.count metric with initial data.
func (m *metricPostgresqlQueryExecutionCount) init() {
	m.data.SetName("postgresql.query.execution.count")
	m.data.SetDescription("Number of times the query was executed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryExecutionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryExecutionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryExecutionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryExecutionCount(cfg MetricConfig) metricPostgresqlQueryExecutionCount {
	m := metricPostgresqlQueryExecutionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRollbacks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rollbacks metric with initial data.
func (m *metricPostgresqlRollbacks) init() {
	m.data.SetName("postgresql.rollbacks")
	m.data.SetDescription("The number of rollbacks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRollbacks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRollbacks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRollbacks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRollbacks(cfg MetricConfig) metricPostgresqlRollbacks {
	m := metricPostgresqlRollbacks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows metric with initial data.
func (m *metricPostgresqlRows) init() {
	m.data.SetName("postgresql.rows")
	m.data.SetDescription("The number of rows.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRows) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRows) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRows(cfg MetricConfig) metricPostgresqlRows {
	m := metricPostgresqlRows{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableScans struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.scans metric with initial data.
func (m *metricPostgresqlTableScans) init() {
	m.data.SetName("postgresql.table.scans")
	m.data.SetDescription("The number of sequential scans on a table.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTableScans) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableScans) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableScans) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableScans(cfg MetricConfig) metricPostgresqlTableScans {
	m := metricPostgresqlTableScans{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.size metric with initial data.
func (m *metricPostgresqlTableSize) init() {
	m.data.SetName("postgresql.table.size")
	m.data.SetDescription("Disk space used by a table.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTableSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableSize(cfg MetricConfig) metricPostgresqlTableSize {
	m := metricPostgresqlTableSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableVacuumCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.vacuum.count metric with initial data.
func (m *metricPostgresqlTableVacuumCount) init() {
	m.data.SetName("postgresql.table.vacuum.count")
	m.data.SetDescription("Number of times a table has been manually vacuumed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTableVacuumCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableVacuumCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableVacuumCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableVacuumCount(cfg MetricConfig) metricPostgresqlTableVacuumCount {
	m := metricPostgresqlTableVacuumCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWaitEventTotalTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wait.event.total_time metric with initial data.
func (m *metricPostgresqlWaitEventTotalTime) init() {
	m.data.SetName("postgresql.wait.event.total_time")
	m.data.SetDescription("Total wait time for the wait event in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWaitEventTotalTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlWaitEventNameAttributeValue string, postgresqlWaitCategoryAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.wait.event.name", postgresqlWaitEventNameAttributeValue)
	dp.Attributes().PutStr("postgresql.wait.category", postgresqlWaitCategoryAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWaitEventTotalTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWaitEventTotalTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWaitEventTotalTime(cfg MetricConfig) metricPostgresqlWaitEventTotalTime {
	m := metricPostgresqlWaitEventTotalTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.age metric with initial data.
func (m *metricPostgresqlWalAge) init() {
	m.data.SetName("postgresql.wal.age")
	m.data.SetDescription("Age of the oldest WAL file.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
}

func (m *metricPostgresqlWalAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalAge(cfg MetricConfig) metricPostgresqlWalAge {
	m := metricPostgresqlWalAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.lag metric with initial data.
func (m *metricPostgresqlWalLag) init() {
	m.data.SetName("postgresql.wal.lag")
	m.data.SetDescription("Time between flushing recent WAL locally and receiving notification that the standby server has completed an operation with it.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
}

func (m *metricPostgresqlWalLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalLag(cfg MetricConfig) metricPostgresqlWalLag {
	m := metricPostgresqlWalLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config                                       MetricsBuilderConfig // config of the metrics builder.
	startTime                                    pcommon.Timestamp    // start time that will be applied to all recorded data points.
	metricsCapacity                              int                  // maximum observed number of metrics per resource.
	metricsBuffer                                pmetric.Metrics      // accumulates metrics data before emitting.
	buildInfo                                    component.BuildInfo  // contains version information.
	resourceAttributeIncludeFilter               map[string]filter.Filter
	resourceAttributeExcludeFilter               map[string]filter.Filter
	metricPostgresqlBlockedSessionPid            metricPostgresqlBlockedSessionPid
	metricPostgresqlBlockingSessionPid           metricPostgresqlBlockingSessionPid
	metricPostgresqlBlocksHit                    metricPostgresqlBlocksHit
	metricPostgresqlBlocksRead                   metricPostgresqlBlocksRead
	metricPostgresqlCommits                      metricPostgresqlCommits
	metricPostgresqlConnectionCount              metricPostgresqlConnectionCount
	metricPostgresqlConnectionMax                metricPostgresqlConnectionMax
	metricPostgresqlDatabaseCount                metricPostgresqlDatabaseCount
	metricPostgresqlDatabaseLocks                metricPostgresqlDatabaseLocks
	metricPostgresqlExecutionPlanActualLoops     metricPostgresqlExecutionPlanActualLoops
	metricPostgresqlExecutionPlanActualRows      metricPostgresqlExecutionPlanActualRows
	metricPostgresqlExecutionPlanActualTotalTime metricPostgresqlExecutionPlanActualTotalTime
	metricPostgresqlExecutionPlanAsyncCapable    metricPostgresqlExecutionPlanAsyncCapable
	metricPostgresqlExecutionPlanParallelAware   metricPostgresqlExecutionPlanParallelAware
	metricPostgresqlIndexScans                   metricPostgresqlIndexScans
	metricPostgresqlIndexSize                    metricPostgresqlIndexSize
	metricPostgresqlOperations                   metricPostgresqlOperations
	metricPostgresqlQueryAvgDiskReads            metricPostgresqlQueryAvgDiskReads
	metricPostgresqlQueryAvgDiskWrites           metricPostgresqlQueryAvgDiskWrites
	metricPostgresqlQueryAvgElapsedTime          metricPostgresqlQueryAvgElapsedTime
	metricPostgresqlQueryCPUTime                 metricPostgresqlQueryCPUTime
	metricPostgresqlQueryExecutionCount          metricPostgresqlQueryExecutionCount
	metricPostgresqlRollbacks                    metricPostgresqlRollbacks
	metricPostgresqlRows                         metricPostgresqlRows
	metricPostgresqlTableScans                   metricPostgresqlTableScans
	metricPostgresqlTableSize                    metricPostgresqlTableSize
	metricPostgresqlTableVacuumCount             metricPostgresqlTableVacuumCount
	metricPostgresqlWaitEventTotalTime           metricPostgresqlWaitEventTotalTime
	metricPostgresqlWalAge                       metricPostgresqlWalAge
	metricPostgresqlWalLag                       metricPostgresqlWalLag
}

// MetricBuilderOption applies changes to default metrics builder.
type MetricBuilderOption interface {
	apply(*MetricsBuilder)
}

type metricBuilderOptionFunc func(mb *MetricsBuilder)

func (mbof metricBuilderOptionFunc) apply(mb *MetricsBuilder) {
	mbof(mb)
}

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) MetricBuilderOption {
	return metricBuilderOptionFunc(func(mb *MetricsBuilder) {
		mb.startTime = startTime
	})
}
func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.Settings, options ...MetricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:                                       mbc,
		startTime:                                    pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                pmetric.NewMetrics(),
		buildInfo:                                    settings.BuildInfo,
		metricPostgresqlBlockedSessionPid:            newMetricPostgresqlBlockedSessionPid(mbc.Metrics.PostgresqlBlockedSessionPid),
		metricPostgresqlBlockingSessionPid:           newMetricPostgresqlBlockingSessionPid(mbc.Metrics.PostgresqlBlockingSessionPid),
		metricPostgresqlBlocksHit:                    newMetricPostgresqlBlocksHit(mbc.Metrics.PostgresqlBlocksHit),
		metricPostgresqlBlocksRead:                   newMetricPostgresqlBlocksRead(mbc.Metrics.PostgresqlBlocksRead),
		metricPostgresqlCommits:                      newMetricPostgresqlCommits(mbc.Metrics.PostgresqlCommits),
		metricPostgresqlConnectionCount:              newMetricPostgresqlConnectionCount(mbc.Metrics.PostgresqlConnectionCount),
		metricPostgresqlConnectionMax:                newMetricPostgresqlConnectionMax(mbc.Metrics.PostgresqlConnectionMax),
		metricPostgresqlDatabaseCount:                newMetricPostgresqlDatabaseCount(mbc.Metrics.PostgresqlDatabaseCount),
		metricPostgresqlDatabaseLocks:                newMetricPostgresqlDatabaseLocks(mbc.Metrics.PostgresqlDatabaseLocks),
		metricPostgresqlExecutionPlanActualLoops:     newMetricPostgresqlExecutionPlanActualLoops(mbc.Metrics.PostgresqlExecutionPlanActualLoops),
		metricPostgresqlExecutionPlanActualRows:      newMetricPostgresqlExecutionPlanActualRows(mbc.Metrics.PostgresqlExecutionPlanActualRows),
		metricPostgresqlExecutionPlanActualTotalTime: newMetricPostgresqlExecutionPlanActualTotalTime(mbc.Metrics.PostgresqlExecutionPlanActualTotalTime),
		metricPostgresqlExecutionPlanAsyncCapable:    newMetricPostgresqlExecutionPlanAsyncCapable(mbc.Metrics.PostgresqlExecutionPlanAsyncCapable),
		metricPostgresqlExecutionPlanParallelAware:   newMetricPostgresqlExecutionPlanParallelAware(mbc.Metrics.PostgresqlExecutionPlanParallelAware),
		metricPostgresqlIndexScans:                   newMetricPostgresqlIndexScans(mbc.Metrics.PostgresqlIndexScans),
		metricPostgresqlIndexSize:                    newMetricPostgresqlIndexSize(mbc.Metrics.PostgresqlIndexSize),
		metricPostgresqlOperations:                   newMetricPostgresqlOperations(mbc.Metrics.PostgresqlOperations),
		metricPostgresqlQueryAvgDiskReads:            newMetricPostgresqlQueryAvgDiskReads(mbc.Metrics.PostgresqlQueryAvgDiskReads),
		metricPostgresqlQueryAvgDiskWrites:           newMetricPostgresqlQueryAvgDiskWrites(mbc.Metrics.PostgresqlQueryAvgDiskWrites),
		metricPostgresqlQueryAvgElapsedTime:          newMetricPostgresqlQueryAvgElapsedTime(mbc.Metrics.PostgresqlQueryAvgElapsedTime),
		metricPostgresqlQueryCPUTime:                 newMetricPostgresqlQueryCPUTime(mbc.Metrics.PostgresqlQueryCPUTime),
		metricPostgresqlQueryExecutionCount:          newMetricPostgresqlQueryExecutionCount(mbc.Metrics.PostgresqlQueryExecutionCount),
		metricPostgresqlRollbacks:                    newMetricPostgresqlRollbacks(mbc.Metrics.PostgresqlRollbacks),
		metricPostgresqlRows:                         newMetricPostgresqlRows(mbc.Metrics.PostgresqlRows),
		metricPostgresqlTableScans:                   newMetricPostgresqlTableScans(mbc.Metrics.PostgresqlTableScans),
		metricPostgresqlTableSize:                    newMetricPostgresqlTableSize(mbc.Metrics.PostgresqlTableSize),
		metricPostgresqlTableVacuumCount:             newMetricPostgresqlTableVacuumCount(mbc.Metrics.PostgresqlTableVacuumCount),
		metricPostgresqlWaitEventTotalTime:           newMetricPostgresqlWaitEventTotalTime(mbc.Metrics.PostgresqlWaitEventTotalTime),
		metricPostgresqlWalAge:                       newMetricPostgresqlWalAge(mbc.Metrics.PostgresqlWalAge),
		metricPostgresqlWalLag:                       newMetricPostgresqlWalLag(mbc.Metrics.PostgresqlWalLag),
		resourceAttributeIncludeFilter:               make(map[string]filter.Filter),
		resourceAttributeExcludeFilter:               make(map[string]filter.Filter),
	}
	if mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.database.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.database.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlIndexName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.index.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlIndexName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlIndexName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.index.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlIndexName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlSchemaName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.schema.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlSchemaName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlSchemaName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.schema.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlSchemaName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlTableName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.table.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlTableName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlTableName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.table.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlTableName.MetricsExclude)
	}

	for _, op := range options {
		op.apply(mb)
	}
	return mb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption interface {
	apply(pmetric.ResourceMetrics)
}

type resourceMetricsOptionFunc func(pmetric.ResourceMetrics)

func (rmof resourceMetricsOptionFunc) apply(rm pmetric.ResourceMetrics) {
	rmof(rm)
}

// WithResource sets the provided resource on the emitted ResourceMetrics.
// It's recommended to use ResourceBuilder to create the resource.
func WithResource(res pcommon.Resource) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		res.CopyTo(rm.Resource())
	})
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	})
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(options ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName(ScopeName)
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricPostgresqlBlockedSessionPid.emit(ils.Metrics())
	mb.metricPostgresqlBlockingSessionPid.emit(ils.Metrics())
	mb.metricPostgresqlBlocksHit.emit(ils.Metrics())
	mb.metricPostgresqlBlocksRead.emit(ils.Metrics())
	mb.metricPostgresqlCommits.emit(ils.Metrics())
	mb.metricPostgresqlConnectionCount.emit(ils.Metrics())
	mb.metricPostgresqlConnectionMax.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseCount.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseLocks.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanActualLoops.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanActualRows.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanActualTotalTime.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanAsyncCapable.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanParallelAware.emit(ils.Metrics())
	mb.metricPostgresqlIndexScans.emit(ils.Metrics())
	mb.metricPostgresqlIndexSize.emit(ils.Metrics())
	mb.metricPostgresqlOperations.emit(ils.Metrics())
	mb.metricPostgresqlQueryAvgDiskReads.emit(ils.Metrics())
	mb.metricPostgresqlQueryAvgDiskWrites.emit(ils.Metrics())
	mb.metricPostgresqlQueryAvgElapsedTime.emit(ils.Metrics())
	mb.metricPostgresqlQueryCPUTime.emit(ils.Metrics())
	mb.metricPostgresqlQueryExecutionCount.emit(ils.Metrics())
	mb.metricPostgresqlRollbacks.emit(ils.Metrics())
	mb.metricPostgresqlRows.emit(ils.Metrics())
	mb.metricPostgresqlTableScans.emit(ils.Metrics())
	mb.metricPostgresqlTableSize.emit(ils.Metrics())
	mb.metricPostgresqlTableVacuumCount.emit(ils.Metrics())
	mb.metricPostgresqlWaitEventTotalTime.emit(ils.Metrics())
	mb.metricPostgresqlWalAge.emit(ils.Metrics())
	mb.metricPostgresqlWalLag.emit(ils.Metrics())

	for _, op := range options {
		op.apply(rm)
	}
	for attr, filter := range mb.resourceAttributeIncludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && !filter.Matches(val.AsString()) {
			return
		}
	}
	for attr, filter := range mb.resourceAttributeExcludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && filter.Matches(val.AsString()) {
			return
		}
	}

	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(options ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(options...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordPostgresqlBlockedSessionPidDataPoint adds a data point to postgresql.blocked.session.pid metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockedSessionPidDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	mb.metricPostgresqlBlockedSessionPid.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryTextAttributeValue)
}

// RecordPostgresqlBlockingSessionPidDataPoint adds a data point to postgresql.blocking.session.pid metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockingSessionPidDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	mb.metricPostgresqlBlockingSessionPid.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryTextAttributeValue)
}

// RecordPostgresqlBlocksHitDataPoint adds a data point to postgresql.blocks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlBlocksHitDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue AttributeSource) {
	mb.metricPostgresqlBlocksHit.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, sourceAttributeValue.String())
}

// RecordPostgresqlBlocksReadDataPoint adds a data point to postgresql.blocks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlBlocksReadDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue AttributeSource) {
	mb.metricPostgresqlBlocksRead.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, sourceAttributeValue.String())
}

// RecordPostgresqlCommitsDataPoint adds a data point to postgresql.commits metric.
func (mb *MetricsBuilder) RecordPostgresqlCommitsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlCommits.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlConnectionCountDataPoint adds a data point to postgresql.connection.count metric.
func (mb *MetricsBuilder) RecordPostgresqlConnectionCountDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlConnectionCount.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlConnectionMaxDataPoint adds a data point to postgresql.connection.max metric.
func (mb *MetricsBuilder) RecordPostgresqlConnectionMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlConnectionMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlDatabaseCountDataPoint adds a data point to postgresql.database.count metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlDatabaseCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlDatabaseLocksDataPoint adds a data point to postgresql.database.locks metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseLocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlDatabaseLocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlExecutionPlanActualLoopsDataPoint adds a data point to postgresql.execution_plan.actual_loops metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanActualLoopsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanActualLoops.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanActualRowsDataPoint adds a data point to postgresql.execution_plan.actual_rows metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanActualRowsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanActualRows.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanActualTotalTimeDataPoint adds a data point to postgresql.execution_plan.actual_total_time metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanActualTotalTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanActualTotalTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanAsyncCapableDataPoint adds a data point to postgresql.execution_plan.async_capable metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanAsyncCapableDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanAsyncCapable.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanParallelAwareDataPoint adds a data point to postgresql.execution_plan.parallel_aware metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanParallelAwareDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanParallelAware.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlIndexScansDataPoint adds a data point to postgresql.index.scans metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexScansDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	mb.metricPostgresqlIndexScans.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue, postgresqlIndexNameAttributeValue)
}

// RecordPostgresqlIndexSizeDataPoint adds a data point to postgresql.index.size metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexSizeDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	mb.metricPostgresqlIndexSize.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue, postgresqlIndexNameAttributeValue)
}

// RecordPostgresqlOperationsDataPoint adds a data point to postgresql.operations metric.
func (mb *MetricsBuilder) RecordPostgresqlOperationsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, operationAttributeValue AttributeOperation) {
	mb.metricPostgresqlOperations.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, operationAttributeValue.String())
}

// RecordPostgresqlQueryAvgDiskReadsDataPoint adds a data point to postgresql.query.avg_disk_reads metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryAvgDiskReadsDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryAvgDiskReads.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlQueryAvgDiskWritesDataPoint adds a data point to postgresql.query.avg_disk_writes metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryAvgDiskWritesDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryAvgDiskWrites.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlQueryAvgElapsedTimeDataPoint adds a data point to postgresql.query.avg_elapsed_time metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryAvgElapsedTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryAvgElapsedTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlQueryCPUTimeDataPoint adds a data point to postgresql.query.cpu_time metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryCPUTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string) {
	mb.metricPostgresqlQueryCPUTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue)
}

// RecordPostgresqlQueryExecutionCountDataPoint adds a data point to postgresql.query.execution.count metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryExecutionCountDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryExecutionCount.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlRollbacksDataPoint adds a data point to postgresql.rollbacks metric.
func (mb *MetricsBuilder) RecordPostgresqlRollbacksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlRollbacks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlRowsDataPoint adds a data point to postgresql.rows metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, stateAttributeValue AttributeState) {
	mb.metricPostgresqlRows.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, stateAttributeValue.String())
}

// RecordPostgresqlTableScansDataPoint adds a data point to postgresql.table.scans metric.
func (mb *MetricsBuilder) RecordPostgresqlTableScansDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	mb.metricPostgresqlTableScans.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue)
}

// RecordPostgresqlTableSizeDataPoint adds a data point to postgresql.table.size metric.
func (mb *MetricsBuilder) RecordPostgresqlTableSizeDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	mb.metricPostgresqlTableSize.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue)
}

// RecordPostgresqlTableVacuumCountDataPoint adds a data point to postgresql.table.vacuum.count metric.
func (mb *MetricsBuilder) RecordPostgresqlTableVacuumCountDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	mb.metricPostgresqlTableVacuumCount.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue)
}

// RecordPostgresqlWaitEventTotalTimeDataPoint adds a data point to postgresql.wait.event.total_time metric.
func (mb *MetricsBuilder) RecordPostgresqlWaitEventTotalTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlWaitEventNameAttributeValue string, postgresqlWaitCategoryAttributeValue string) {
	mb.metricPostgresqlWaitEventTotalTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlWaitEventNameAttributeValue, postgresqlWaitCategoryAttributeValue)
}

// RecordPostgresqlWalAgeDataPoint adds a data point to postgresql.wal.age metric.
func (mb *MetricsBuilder) RecordPostgresqlWalAgeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlWalAge.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlWalLagDataPoint adds a data point to postgresql.wal.lag metric.
func (mb *MetricsBuilder) RecordPostgresqlWalLagDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricPostgresqlWalLag.recordDataPoint(mb.startTime, ts, val)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...MetricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op.apply(mb)
	}
}
