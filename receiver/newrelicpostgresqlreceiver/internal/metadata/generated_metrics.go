// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/filter"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"
)

// AttributeBgBufferSource specifies the value bg_buffer_source attribute.
type AttributeBgBufferSource int

const (
	_ AttributeBgBufferSource = iota
	AttributeBgBufferSourceBackend
	AttributeBgBufferSourceBackendFsync
	AttributeBgBufferSourceCheckpoints
	AttributeBgBufferSourceBgwriter
)

// String returns the string representation of the AttributeBgBufferSource.
func (av AttributeBgBufferSource) String() string {
	switch av {
	case AttributeBgBufferSourceBackend:
		return "backend"
	case AttributeBgBufferSourceBackendFsync:
		return "backend_fsync"
	case AttributeBgBufferSourceCheckpoints:
		return "checkpoints"
	case AttributeBgBufferSourceBgwriter:
		return "bgwriter"
	}
	return ""
}

// MapAttributeBgBufferSource is a helper map of string to AttributeBgBufferSource attribute value.
var MapAttributeBgBufferSource = map[string]AttributeBgBufferSource{
	"backend":       AttributeBgBufferSourceBackend,
	"backend_fsync": AttributeBgBufferSourceBackendFsync,
	"checkpoints":   AttributeBgBufferSourceCheckpoints,
	"bgwriter":      AttributeBgBufferSourceBgwriter,
}

// AttributeBgCheckpointType specifies the value bg_checkpoint_type attribute.
type AttributeBgCheckpointType int

const (
	_ AttributeBgCheckpointType = iota
	AttributeBgCheckpointTypeRequested
	AttributeBgCheckpointTypeScheduled
)

// String returns the string representation of the AttributeBgCheckpointType.
func (av AttributeBgCheckpointType) String() string {
	switch av {
	case AttributeBgCheckpointTypeRequested:
		return "requested"
	case AttributeBgCheckpointTypeScheduled:
		return "scheduled"
	}
	return ""
}

// MapAttributeBgCheckpointType is a helper map of string to AttributeBgCheckpointType attribute value.
var MapAttributeBgCheckpointType = map[string]AttributeBgCheckpointType{
	"requested": AttributeBgCheckpointTypeRequested,
	"scheduled": AttributeBgCheckpointTypeScheduled,
}

// AttributeBgDurationType specifies the value bg_duration_type attribute.
type AttributeBgDurationType int

const (
	_ AttributeBgDurationType = iota
	AttributeBgDurationTypeSync
	AttributeBgDurationTypeWrite
)

// String returns the string representation of the AttributeBgDurationType.
func (av AttributeBgDurationType) String() string {
	switch av {
	case AttributeBgDurationTypeSync:
		return "sync"
	case AttributeBgDurationTypeWrite:
		return "write"
	}
	return ""
}

// MapAttributeBgDurationType is a helper map of string to AttributeBgDurationType attribute value.
var MapAttributeBgDurationType = map[string]AttributeBgDurationType{
	"sync":  AttributeBgDurationTypeSync,
	"write": AttributeBgDurationTypeWrite,
}

// AttributeOperation specifies the value operation attribute.
type AttributeOperation int

const (
	_ AttributeOperation = iota
	AttributeOperationIns
	AttributeOperationUpd
	AttributeOperationDel
	AttributeOperationHotUpd
)

// String returns the string representation of the AttributeOperation.
func (av AttributeOperation) String() string {
	switch av {
	case AttributeOperationIns:
		return "ins"
	case AttributeOperationUpd:
		return "upd"
	case AttributeOperationDel:
		return "del"
	case AttributeOperationHotUpd:
		return "hot_upd"
	}
	return ""
}

// MapAttributeOperation is a helper map of string to AttributeOperation attribute value.
var MapAttributeOperation = map[string]AttributeOperation{
	"ins":     AttributeOperationIns,
	"upd":     AttributeOperationUpd,
	"del":     AttributeOperationDel,
	"hot_upd": AttributeOperationHotUpd,
}

// AttributeSource specifies the value source attribute.
type AttributeSource int

const (
	_ AttributeSource = iota
	AttributeSourceHeapRead
	AttributeSourceHeapHit
	AttributeSourceIdxRead
	AttributeSourceIdxHit
	AttributeSourceToastRead
	AttributeSourceToastHit
	AttributeSourceTidxRead
	AttributeSourceTidxHit
)

// String returns the string representation of the AttributeSource.
func (av AttributeSource) String() string {
	switch av {
	case AttributeSourceHeapRead:
		return "heap_read"
	case AttributeSourceHeapHit:
		return "heap_hit"
	case AttributeSourceIdxRead:
		return "idx_read"
	case AttributeSourceIdxHit:
		return "idx_hit"
	case AttributeSourceToastRead:
		return "toast_read"
	case AttributeSourceToastHit:
		return "toast_hit"
	case AttributeSourceTidxRead:
		return "tidx_read"
	case AttributeSourceTidxHit:
		return "tidx_hit"
	}
	return ""
}

// MapAttributeSource is a helper map of string to AttributeSource attribute value.
var MapAttributeSource = map[string]AttributeSource{
	"heap_read":  AttributeSourceHeapRead,
	"heap_hit":   AttributeSourceHeapHit,
	"idx_read":   AttributeSourceIdxRead,
	"idx_hit":    AttributeSourceIdxHit,
	"toast_read": AttributeSourceToastRead,
	"toast_hit":  AttributeSourceToastHit,
	"tidx_read":  AttributeSourceTidxRead,
	"tidx_hit":   AttributeSourceTidxHit,
}

// AttributeState specifies the value state attribute.
type AttributeState int

const (
	_ AttributeState = iota
	AttributeStateDead
	AttributeStateLive
)

// String returns the string representation of the AttributeState.
func (av AttributeState) String() string {
	switch av {
	case AttributeStateDead:
		return "dead"
	case AttributeStateLive:
		return "live"
	}
	return ""
}

// MapAttributeState is a helper map of string to AttributeState attribute value.
var MapAttributeState = map[string]AttributeState{
	"dead": AttributeStateDead,
	"live": AttributeStateLive,
}

var MetricsInfo = metricsInfo{
	PostgresqlBackends: metricInfo{
		Name: "postgresql.backends",
	},
	PostgresqlBgwriterBuffersAllocated: metricInfo{
		Name: "postgresql.bgwriter.buffers.allocated",
	},
	PostgresqlBgwriterBuffersWrites: metricInfo{
		Name: "postgresql.bgwriter.buffers.writes",
	},
	PostgresqlBgwriterCheckpointCount: metricInfo{
		Name: "postgresql.bgwriter.checkpoint.count",
	},
	PostgresqlBgwriterDuration: metricInfo{
		Name: "postgresql.bgwriter.duration",
	},
	PostgresqlBgwriterMaxwritten: metricInfo{
		Name: "postgresql.bgwriter.maxwritten",
	},
	PostgresqlBlksHit: metricInfo{
		Name: "postgresql.blks_hit",
	},
	PostgresqlBlksRead: metricInfo{
		Name: "postgresql.blks_read",
	},
	PostgresqlBlockedSessionPid: metricInfo{
		Name: "postgresql.blocked.session.pid",
	},
	PostgresqlBlockingSessionDuration: metricInfo{
		Name: "postgresql.blocking.session.duration",
	},
	PostgresqlBlockingSessionPid: metricInfo{
		Name: "postgresql.blocking.session.pid",
	},
	PostgresqlBlockingSessionWaitEvent: metricInfo{
		Name: "postgresql.blocking.session.wait_event",
	},
	PostgresqlBlockingSessionWaitEventType: metricInfo{
		Name: "postgresql.blocking.session.wait_event_type",
	},
	PostgresqlBlocksHit: metricInfo{
		Name: "postgresql.blocks_hit",
	},
	PostgresqlBlocksRead: metricInfo{
		Name: "postgresql.blocks_read",
	},
	PostgresqlCommits: metricInfo{
		Name: "postgresql.commits",
	},
	PostgresqlConnectionCount: metricInfo{
		Name: "postgresql.connection.count",
	},
	PostgresqlConnectionMax: metricInfo{
		Name: "postgresql.connection.max",
	},
	PostgresqlDatabaseCount: metricInfo{
		Name: "postgresql.database.count",
	},
	PostgresqlDatabaseLocks: metricInfo{
		Name: "postgresql.database.locks",
	},
	PostgresqlDbSize: metricInfo{
		Name: "postgresql.db_size",
	},
	PostgresqlDeadlocks: metricInfo{
		Name: "postgresql.deadlocks",
	},
	PostgresqlExecutionPlanActualLoops: metricInfo{
		Name: "postgresql.execution_plan.actual_loops",
	},
	PostgresqlExecutionPlanActualRows: metricInfo{
		Name: "postgresql.execution_plan.actual_rows",
	},
	PostgresqlExecutionPlanActualTotalTime: metricInfo{
		Name: "postgresql.execution_plan.actual_total_time",
	},
	PostgresqlExecutionPlanAsyncCapable: metricInfo{
		Name: "postgresql.execution_plan.async_capable",
	},
	PostgresqlExecutionPlanCostActual: metricInfo{
		Name: "postgresql.execution_plan.cost_actual",
	},
	PostgresqlExecutionPlanCostEstimate: metricInfo{
		Name: "postgresql.execution_plan.cost_estimate",
	},
	PostgresqlExecutionPlanIoReadTime: metricInfo{
		Name: "postgresql.execution_plan.io_read_time",
	},
	PostgresqlExecutionPlanIoWriteTime: metricInfo{
		Name: "postgresql.execution_plan.io_write_time",
	},
	PostgresqlExecutionPlanParallelAware: metricInfo{
		Name: "postgresql.execution_plan.parallel_aware",
	},
	PostgresqlExecutionPlanPlanRows: metricInfo{
		Name: "postgresql.execution_plan.plan_rows",
	},
	PostgresqlExecutionPlanPlanWidth: metricInfo{
		Name: "postgresql.execution_plan.plan_width",
	},
	PostgresqlExecutionPlanSharedHitBlocks: metricInfo{
		Name: "postgresql.execution_plan.shared_hit_blocks",
	},
	PostgresqlExecutionPlanSharedReadBlocks: metricInfo{
		Name: "postgresql.execution_plan.shared_read_blocks",
	},
	PostgresqlExecutionPlanSharedWrittenBlocks: metricInfo{
		Name: "postgresql.execution_plan.shared_written_blocks",
	},
	PostgresqlExecutionPlanStartupTime: metricInfo{
		Name: "postgresql.execution_plan.startup_time",
	},
	PostgresqlExecutionPlanTempReadBlocks: metricInfo{
		Name: "postgresql.execution_plan.temp_read_blocks",
	},
	PostgresqlExecutionPlanTempWrittenBlocks: metricInfo{
		Name: "postgresql.execution_plan.temp_written_blocks",
	},
	PostgresqlIndexScans: metricInfo{
		Name: "postgresql.index.scans",
	},
	PostgresqlIndexSize: metricInfo{
		Name: "postgresql.index.size",
	},
	PostgresqlOperations: metricInfo{
		Name: "postgresql.operations",
	},
	PostgresqlQueryAvgDiskReads: metricInfo{
		Name: "postgresql.query.avg_disk_reads",
	},
	PostgresqlQueryAvgDiskWrites: metricInfo{
		Name: "postgresql.query.avg_disk_writes",
	},
	PostgresqlQueryAvgElapsedTime: metricInfo{
		Name: "postgresql.query.avg_elapsed_time",
	},
	PostgresqlQueryCPUTime: metricInfo{
		Name: "postgresql.query.cpu_time",
	},
	PostgresqlQueryExecutionCount: metricInfo{
		Name: "postgresql.query.execution.count",
	},
	PostgresqlReplicationDataDelay: metricInfo{
		Name: "postgresql.replication.data_delay",
	},
	PostgresqlRollbacks: metricInfo{
		Name: "postgresql.rollbacks",
	},
	PostgresqlRows: metricInfo{
		Name: "postgresql.rows",
	},
	PostgresqlSequentialScans: metricInfo{
		Name: "postgresql.sequential_scans",
	},
	PostgresqlTableCount: metricInfo{
		Name: "postgresql.table.count",
	},
	PostgresqlTableScans: metricInfo{
		Name: "postgresql.table.scans",
	},
	PostgresqlTableSize: metricInfo{
		Name: "postgresql.table.size",
	},
	PostgresqlTableVacuumCount: metricInfo{
		Name: "postgresql.table.vacuum.count",
	},
	PostgresqlTempFiles: metricInfo{
		Name: "postgresql.temp_files",
	},
	PostgresqlTupDeleted: metricInfo{
		Name: "postgresql.tup_deleted",
	},
	PostgresqlTupFetched: metricInfo{
		Name: "postgresql.tup_fetched",
	},
	PostgresqlTupInserted: metricInfo{
		Name: "postgresql.tup_inserted",
	},
	PostgresqlTupReturned: metricInfo{
		Name: "postgresql.tup_returned",
	},
	PostgresqlTupUpdated: metricInfo{
		Name: "postgresql.tup_updated",
	},
	PostgresqlWaitEventTotalTime: metricInfo{
		Name: "postgresql.wait.event.total_time",
	},
	PostgresqlWalAge: metricInfo{
		Name: "postgresql.wal.age",
	},
	PostgresqlWalDelay: metricInfo{
		Name: "postgresql.wal.delay",
	},
	PostgresqlWalLag: metricInfo{
		Name: "postgresql.wal.lag",
	},
}

type metricsInfo struct {
	PostgresqlBackends                         metricInfo
	PostgresqlBgwriterBuffersAllocated         metricInfo
	PostgresqlBgwriterBuffersWrites            metricInfo
	PostgresqlBgwriterCheckpointCount          metricInfo
	PostgresqlBgwriterDuration                 metricInfo
	PostgresqlBgwriterMaxwritten               metricInfo
	PostgresqlBlksHit                          metricInfo
	PostgresqlBlksRead                         metricInfo
	PostgresqlBlockedSessionPid                metricInfo
	PostgresqlBlockingSessionDuration          metricInfo
	PostgresqlBlockingSessionPid               metricInfo
	PostgresqlBlockingSessionWaitEvent         metricInfo
	PostgresqlBlockingSessionWaitEventType     metricInfo
	PostgresqlBlocksHit                        metricInfo
	PostgresqlBlocksRead                       metricInfo
	PostgresqlCommits                          metricInfo
	PostgresqlConnectionCount                  metricInfo
	PostgresqlConnectionMax                    metricInfo
	PostgresqlDatabaseCount                    metricInfo
	PostgresqlDatabaseLocks                    metricInfo
	PostgresqlDbSize                           metricInfo
	PostgresqlDeadlocks                        metricInfo
	PostgresqlExecutionPlanActualLoops         metricInfo
	PostgresqlExecutionPlanActualRows          metricInfo
	PostgresqlExecutionPlanActualTotalTime     metricInfo
	PostgresqlExecutionPlanAsyncCapable        metricInfo
	PostgresqlExecutionPlanCostActual          metricInfo
	PostgresqlExecutionPlanCostEstimate        metricInfo
	PostgresqlExecutionPlanIoReadTime          metricInfo
	PostgresqlExecutionPlanIoWriteTime         metricInfo
	PostgresqlExecutionPlanParallelAware       metricInfo
	PostgresqlExecutionPlanPlanRows            metricInfo
	PostgresqlExecutionPlanPlanWidth           metricInfo
	PostgresqlExecutionPlanSharedHitBlocks     metricInfo
	PostgresqlExecutionPlanSharedReadBlocks    metricInfo
	PostgresqlExecutionPlanSharedWrittenBlocks metricInfo
	PostgresqlExecutionPlanStartupTime         metricInfo
	PostgresqlExecutionPlanTempReadBlocks      metricInfo
	PostgresqlExecutionPlanTempWrittenBlocks   metricInfo
	PostgresqlIndexScans                       metricInfo
	PostgresqlIndexSize                        metricInfo
	PostgresqlOperations                       metricInfo
	PostgresqlQueryAvgDiskReads                metricInfo
	PostgresqlQueryAvgDiskWrites               metricInfo
	PostgresqlQueryAvgElapsedTime              metricInfo
	PostgresqlQueryCPUTime                     metricInfo
	PostgresqlQueryExecutionCount              metricInfo
	PostgresqlReplicationDataDelay             metricInfo
	PostgresqlRollbacks                        metricInfo
	PostgresqlRows                             metricInfo
	PostgresqlSequentialScans                  metricInfo
	PostgresqlTableCount                       metricInfo
	PostgresqlTableScans                       metricInfo
	PostgresqlTableSize                        metricInfo
	PostgresqlTableVacuumCount                 metricInfo
	PostgresqlTempFiles                        metricInfo
	PostgresqlTupDeleted                       metricInfo
	PostgresqlTupFetched                       metricInfo
	PostgresqlTupInserted                      metricInfo
	PostgresqlTupReturned                      metricInfo
	PostgresqlTupUpdated                       metricInfo
	PostgresqlWaitEventTotalTime               metricInfo
	PostgresqlWalAge                           metricInfo
	PostgresqlWalDelay                         metricInfo
	PostgresqlWalLag                           metricInfo
}

type metricInfo struct {
	Name string
}

type metricPostgresqlBackends struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.backends metric with initial data.
func (m *metricPostgresqlBackends) init() {
	m.data.SetName("postgresql.backends")
	m.data.SetDescription("The number of backends.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricPostgresqlBackends) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBackends) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBackends) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBackends(cfg MetricConfig) metricPostgresqlBackends {
	m := metricPostgresqlBackends{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersAllocated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers.allocated metric with initial data.
func (m *metricPostgresqlBgwriterBuffersAllocated) init() {
	m.data.SetName("postgresql.bgwriter.buffers.allocated")
	m.data.SetDescription("Number of buffers allocated.")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricPostgresqlBgwriterBuffersAllocated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersAllocated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersAllocated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersAllocated(cfg MetricConfig) metricPostgresqlBgwriterBuffersAllocated {
	m := metricPostgresqlBgwriterBuffersAllocated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterBuffersWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.buffers.writes metric with initial data.
func (m *metricPostgresqlBgwriterBuffersWrites) init() {
	m.data.SetName("postgresql.bgwriter.buffers.writes")
	m.data.SetDescription("Number of buffers written.")
	m.data.SetUnit("{buffers}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterBuffersWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, bgBufferSourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("source", bgBufferSourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterBuffersWrites) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterBuffersWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterBuffersWrites(cfg MetricConfig) metricPostgresqlBgwriterBuffersWrites {
	m := metricPostgresqlBgwriterBuffersWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterCheckpointCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.checkpoint.count metric with initial data.
func (m *metricPostgresqlBgwriterCheckpointCount) init() {
	m.data.SetName("postgresql.bgwriter.checkpoint.count")
	m.data.SetDescription("The number of checkpoints performed.")
	m.data.SetUnit("{checkpoints}")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterCheckpointCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, bgCheckpointTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("type", bgCheckpointTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterCheckpointCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterCheckpointCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterCheckpointCount(cfg MetricConfig) metricPostgresqlBgwriterCheckpointCount {
	m := metricPostgresqlBgwriterCheckpointCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterDuration struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.duration metric with initial data.
func (m *metricPostgresqlBgwriterDuration) init() {
	m.data.SetName("postgresql.bgwriter.duration")
	m.data.SetDescription("Total time spent writing and syncing files to disk by checkpoints.")
	m.data.SetUnit("ms")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBgwriterDuration) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, bgDurationTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("type", bgDurationTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterDuration) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterDuration) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterDuration(cfg MetricConfig) metricPostgresqlBgwriterDuration {
	m := metricPostgresqlBgwriterDuration{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBgwriterMaxwritten struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.bgwriter.maxwritten metric with initial data.
func (m *metricPostgresqlBgwriterMaxwritten) init() {
	m.data.SetName("postgresql.bgwriter.maxwritten")
	m.data.SetDescription("Number of times the background writer stopped a cleaning scan because it had written too many buffers.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
}

func (m *metricPostgresqlBgwriterMaxwritten) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBgwriterMaxwritten) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBgwriterMaxwritten) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBgwriterMaxwritten(cfg MetricConfig) metricPostgresqlBgwriterMaxwritten {
	m := metricPostgresqlBgwriterMaxwritten{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blks_hit metric with initial data.
func (m *metricPostgresqlBlksHit) init() {
	m.data.SetName("postgresql.blks_hit")
	m.data.SetDescription("Number of times disk blocks were found already in the buffer cache.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlksHit) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlksHit(cfg MetricConfig) metricPostgresqlBlksHit {
	m := metricPostgresqlBlksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blks_read metric with initial data.
func (m *metricPostgresqlBlksRead) init() {
	m.data.SetName("postgresql.blks_read")
	m.data.SetDescription("Number of disk blocks read in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlksRead) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlksRead(cfg MetricConfig) metricPostgresqlBlksRead {
	m := metricPostgresqlBlksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlockedSessionPid struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocked.session.pid metric with initial data.
func (m *metricPostgresqlBlockedSessionPid) init() {
	m.data.SetName("postgresql.blocked.session.pid")
	m.data.SetDescription("Process ID of the blocked session.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlockedSessionPid) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockedSessionPid) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockedSessionPid) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockedSessionPid(cfg MetricConfig) metricPostgresqlBlockedSessionPid {
	m := metricPostgresqlBlockedSessionPid{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlockingSessionDuration struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocking.session.duration metric with initial data.
func (m *metricPostgresqlBlockingSessionDuration) init() {
	m.data.SetName("postgresql.blocking.session.duration")
	m.data.SetDescription("Duration for which the session has been blocking.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlockingSessionDuration) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlBlockedQueryTextAttributeValue string, postgresqlBlockingQueryTextAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.blocked.query.text", postgresqlBlockedQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.blocking.query.text", postgresqlBlockingQueryTextAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockingSessionDuration) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockingSessionDuration) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockingSessionDuration(cfg MetricConfig) metricPostgresqlBlockingSessionDuration {
	m := metricPostgresqlBlockingSessionDuration{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlockingSessionPid struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocking.session.pid metric with initial data.
func (m *metricPostgresqlBlockingSessionPid) init() {
	m.data.SetName("postgresql.blocking.session.pid")
	m.data.SetDescription("Process ID of the blocking session.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlockingSessionPid) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockingSessionPid) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockingSessionPid) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockingSessionPid(cfg MetricConfig) metricPostgresqlBlockingSessionPid {
	m := metricPostgresqlBlockingSessionPid{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlockingSessionWaitEvent struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocking.session.wait_event metric with initial data.
func (m *metricPostgresqlBlockingSessionWaitEvent) init() {
	m.data.SetName("postgresql.blocking.session.wait_event")
	m.data.SetDescription("Wait event for the blocking session.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlockingSessionWaitEvent) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlBlockedQueryTextAttributeValue string, postgresqlBlockingQueryTextAttributeValue string, postgresqlWaitEventAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.blocked.query.text", postgresqlBlockedQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.blocking.query.text", postgresqlBlockingQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.wait.event", postgresqlWaitEventAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockingSessionWaitEvent) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockingSessionWaitEvent) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockingSessionWaitEvent(cfg MetricConfig) metricPostgresqlBlockingSessionWaitEvent {
	m := metricPostgresqlBlockingSessionWaitEvent{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlockingSessionWaitEventType struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocking.session.wait_event_type metric with initial data.
func (m *metricPostgresqlBlockingSessionWaitEventType) init() {
	m.data.SetName("postgresql.blocking.session.wait_event_type")
	m.data.SetDescription("Wait event type for the blocking session.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlockingSessionWaitEventType) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlBlockedQueryTextAttributeValue string, postgresqlBlockingQueryTextAttributeValue string, postgresqlWaitEventTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.blocked.query.text", postgresqlBlockedQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.blocking.query.text", postgresqlBlockingQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.wait.event.type", postgresqlWaitEventTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlockingSessionWaitEventType) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlockingSessionWaitEventType) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlockingSessionWaitEventType(cfg MetricConfig) metricPostgresqlBlockingSessionWaitEventType {
	m := metricPostgresqlBlockingSessionWaitEventType{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlocksHit struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocks_hit metric with initial data.
func (m *metricPostgresqlBlocksHit) init() {
	m.data.SetName("postgresql.blocks_hit")
	m.data.SetDescription("The number of blocks found in the buffer cache.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlocksHit) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("source", sourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlocksHit) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlocksHit) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlocksHit(cfg MetricConfig) metricPostgresqlBlocksHit {
	m := metricPostgresqlBlocksHit{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlBlocksRead struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.blocks_read metric with initial data.
func (m *metricPostgresqlBlocksRead) init() {
	m.data.SetName("postgresql.blocks_read")
	m.data.SetDescription("The number of blocks read.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlBlocksRead) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("source", sourceAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlBlocksRead) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlBlocksRead) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlBlocksRead(cfg MetricConfig) metricPostgresqlBlocksRead {
	m := metricPostgresqlBlocksRead{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlCommits struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.commits metric with initial data.
func (m *metricPostgresqlCommits) init() {
	m.data.SetName("postgresql.commits")
	m.data.SetDescription("The number of commits.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlCommits) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlCommits) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlCommits) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlCommits(cfg MetricConfig) metricPostgresqlCommits {
	m := metricPostgresqlCommits{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConnectionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.connection.count metric with initial data.
func (m *metricPostgresqlConnectionCount) init() {
	m.data.SetName("postgresql.connection.count")
	m.data.SetDescription("Number of user connections.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlConnectionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConnectionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConnectionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConnectionCount(cfg MetricConfig) metricPostgresqlConnectionCount {
	m := metricPostgresqlConnectionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlConnectionMax struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.connection.max metric with initial data.
func (m *metricPostgresqlConnectionMax) init() {
	m.data.SetName("postgresql.connection.max")
	m.data.SetDescription("Maximum number of client connections allowed.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
}

func (m *metricPostgresqlConnectionMax) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlConnectionMax) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlConnectionMax) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlConnectionMax(cfg MetricConfig) metricPostgresqlConnectionMax {
	m := metricPostgresqlConnectionMax{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database.count metric with initial data.
func (m *metricPostgresqlDatabaseCount) init() {
	m.data.SetName("postgresql.database.count")
	m.data.SetDescription("Number of user databases.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
}

func (m *metricPostgresqlDatabaseCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseCount(cfg MetricConfig) metricPostgresqlDatabaseCount {
	m := metricPostgresqlDatabaseCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDatabaseLocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.database.locks metric with initial data.
func (m *metricPostgresqlDatabaseLocks) init() {
	m.data.SetName("postgresql.database.locks")
	m.data.SetDescription("The number of database locks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDatabaseLocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDatabaseLocks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDatabaseLocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDatabaseLocks(cfg MetricConfig) metricPostgresqlDatabaseLocks {
	m := metricPostgresqlDatabaseLocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDbSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.db_size metric with initial data.
func (m *metricPostgresqlDbSize) init() {
	m.data.SetName("postgresql.db_size")
	m.data.SetDescription("The database disk usage.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDbSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDbSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDbSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDbSize(cfg MetricConfig) metricPostgresqlDbSize {
	m := metricPostgresqlDbSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlDeadlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.deadlocks metric with initial data.
func (m *metricPostgresqlDeadlocks) init() {
	m.data.SetName("postgresql.deadlocks")
	m.data.SetDescription("Number of deadlocks detected in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlDeadlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlDeadlocks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlDeadlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlDeadlocks(cfg MetricConfig) metricPostgresqlDeadlocks {
	m := metricPostgresqlDeadlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanActualLoops struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.actual_loops metric with initial data.
func (m *metricPostgresqlExecutionPlanActualLoops) init() {
	m.data.SetName("postgresql.execution_plan.actual_loops")
	m.data.SetDescription("Number of times the execution plan node was executed.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanActualLoops) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanActualLoops) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanActualLoops) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanActualLoops(cfg MetricConfig) metricPostgresqlExecutionPlanActualLoops {
	m := metricPostgresqlExecutionPlanActualLoops{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanActualRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.actual_rows metric with initial data.
func (m *metricPostgresqlExecutionPlanActualRows) init() {
	m.data.SetName("postgresql.execution_plan.actual_rows")
	m.data.SetDescription("Actual number of rows processed by the execution plan node.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanActualRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanActualRows) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanActualRows) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanActualRows(cfg MetricConfig) metricPostgresqlExecutionPlanActualRows {
	m := metricPostgresqlExecutionPlanActualRows{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanActualTotalTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.actual_total_time metric with initial data.
func (m *metricPostgresqlExecutionPlanActualTotalTime) init() {
	m.data.SetName("postgresql.execution_plan.actual_total_time")
	m.data.SetDescription("Total time spent in the execution plan node in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanActualTotalTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanActualTotalTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanActualTotalTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanActualTotalTime(cfg MetricConfig) metricPostgresqlExecutionPlanActualTotalTime {
	m := metricPostgresqlExecutionPlanActualTotalTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanAsyncCapable struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.async_capable metric with initial data.
func (m *metricPostgresqlExecutionPlanAsyncCapable) init() {
	m.data.SetName("postgresql.execution_plan.async_capable")
	m.data.SetDescription("Whether the execution plan node is async capable.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanAsyncCapable) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanAsyncCapable) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanAsyncCapable) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanAsyncCapable(cfg MetricConfig) metricPostgresqlExecutionPlanAsyncCapable {
	m := metricPostgresqlExecutionPlanAsyncCapable{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanCostActual struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.cost_actual metric with initial data.
func (m *metricPostgresqlExecutionPlanCostActual) init() {
	m.data.SetName("postgresql.execution_plan.cost_actual")
	m.data.SetDescription("Actual cost of the execution plan node.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanCostActual) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanCostActual) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanCostActual) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanCostActual(cfg MetricConfig) metricPostgresqlExecutionPlanCostActual {
	m := metricPostgresqlExecutionPlanCostActual{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanCostEstimate struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.cost_estimate metric with initial data.
func (m *metricPostgresqlExecutionPlanCostEstimate) init() {
	m.data.SetName("postgresql.execution_plan.cost_estimate")
	m.data.SetDescription("Estimated cost of the execution plan node.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanCostEstimate) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanCostEstimate) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanCostEstimate) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanCostEstimate(cfg MetricConfig) metricPostgresqlExecutionPlanCostEstimate {
	m := metricPostgresqlExecutionPlanCostEstimate{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanIoReadTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.io_read_time metric with initial data.
func (m *metricPostgresqlExecutionPlanIoReadTime) init() {
	m.data.SetName("postgresql.execution_plan.io_read_time")
	m.data.SetDescription("Time spent reading blocks from disk in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanIoReadTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanIoReadTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanIoReadTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanIoReadTime(cfg MetricConfig) metricPostgresqlExecutionPlanIoReadTime {
	m := metricPostgresqlExecutionPlanIoReadTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanIoWriteTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.io_write_time metric with initial data.
func (m *metricPostgresqlExecutionPlanIoWriteTime) init() {
	m.data.SetName("postgresql.execution_plan.io_write_time")
	m.data.SetDescription("Time spent writing blocks to disk in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanIoWriteTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanIoWriteTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanIoWriteTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanIoWriteTime(cfg MetricConfig) metricPostgresqlExecutionPlanIoWriteTime {
	m := metricPostgresqlExecutionPlanIoWriteTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanParallelAware struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.parallel_aware metric with initial data.
func (m *metricPostgresqlExecutionPlanParallelAware) init() {
	m.data.SetName("postgresql.execution_plan.parallel_aware")
	m.data.SetDescription("Whether the execution plan node is parallel aware.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanParallelAware) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanParallelAware) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanParallelAware) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanParallelAware(cfg MetricConfig) metricPostgresqlExecutionPlanParallelAware {
	m := metricPostgresqlExecutionPlanParallelAware{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanPlanRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.plan_rows metric with initial data.
func (m *metricPostgresqlExecutionPlanPlanRows) init() {
	m.data.SetName("postgresql.execution_plan.plan_rows")
	m.data.SetDescription("Estimated number of rows to be processed by the execution plan node.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanPlanRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanPlanRows) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanPlanRows) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanPlanRows(cfg MetricConfig) metricPostgresqlExecutionPlanPlanRows {
	m := metricPostgresqlExecutionPlanPlanRows{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanPlanWidth struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.plan_width metric with initial data.
func (m *metricPostgresqlExecutionPlanPlanWidth) init() {
	m.data.SetName("postgresql.execution_plan.plan_width")
	m.data.SetDescription("Estimated width of rows to be processed by the execution plan node.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanPlanWidth) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanPlanWidth) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanPlanWidth) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanPlanWidth(cfg MetricConfig) metricPostgresqlExecutionPlanPlanWidth {
	m := metricPostgresqlExecutionPlanPlanWidth{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanSharedHitBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.shared_hit_blocks metric with initial data.
func (m *metricPostgresqlExecutionPlanSharedHitBlocks) init() {
	m.data.SetName("postgresql.execution_plan.shared_hit_blocks")
	m.data.SetDescription("Number of shared blocks hit from cache.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanSharedHitBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanSharedHitBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanSharedHitBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanSharedHitBlocks(cfg MetricConfig) metricPostgresqlExecutionPlanSharedHitBlocks {
	m := metricPostgresqlExecutionPlanSharedHitBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanSharedReadBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.shared_read_blocks metric with initial data.
func (m *metricPostgresqlExecutionPlanSharedReadBlocks) init() {
	m.data.SetName("postgresql.execution_plan.shared_read_blocks")
	m.data.SetDescription("Number of shared blocks read from disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanSharedReadBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanSharedReadBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanSharedReadBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanSharedReadBlocks(cfg MetricConfig) metricPostgresqlExecutionPlanSharedReadBlocks {
	m := metricPostgresqlExecutionPlanSharedReadBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanSharedWrittenBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.shared_written_blocks metric with initial data.
func (m *metricPostgresqlExecutionPlanSharedWrittenBlocks) init() {
	m.data.SetName("postgresql.execution_plan.shared_written_blocks")
	m.data.SetDescription("Number of shared blocks written to disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanSharedWrittenBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanSharedWrittenBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanSharedWrittenBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanSharedWrittenBlocks(cfg MetricConfig) metricPostgresqlExecutionPlanSharedWrittenBlocks {
	m := metricPostgresqlExecutionPlanSharedWrittenBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanStartupTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.startup_time metric with initial data.
func (m *metricPostgresqlExecutionPlanStartupTime) init() {
	m.data.SetName("postgresql.execution_plan.startup_time")
	m.data.SetDescription("Time taken to start the execution plan node in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanStartupTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanStartupTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanStartupTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanStartupTime(cfg MetricConfig) metricPostgresqlExecutionPlanStartupTime {
	m := metricPostgresqlExecutionPlanStartupTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanTempReadBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.temp_read_blocks metric with initial data.
func (m *metricPostgresqlExecutionPlanTempReadBlocks) init() {
	m.data.SetName("postgresql.execution_plan.temp_read_blocks")
	m.data.SetDescription("Number of temporary blocks read from disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanTempReadBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanTempReadBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanTempReadBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanTempReadBlocks(cfg MetricConfig) metricPostgresqlExecutionPlanTempReadBlocks {
	m := metricPostgresqlExecutionPlanTempReadBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlExecutionPlanTempWrittenBlocks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.execution_plan.temp_written_blocks metric with initial data.
func (m *metricPostgresqlExecutionPlanTempWrittenBlocks) init() {
	m.data.SetName("postgresql.execution_plan.temp_written_blocks")
	m.data.SetDescription("Number of temporary blocks written to disk.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlExecutionPlanTempWrittenBlocks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.node.type", postgresqlNodeTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlExecutionPlanTempWrittenBlocks) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlExecutionPlanTempWrittenBlocks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlExecutionPlanTempWrittenBlocks(cfg MetricConfig) metricPostgresqlExecutionPlanTempWrittenBlocks {
	m := metricPostgresqlExecutionPlanTempWrittenBlocks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexScans struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index.scans metric with initial data.
func (m *metricPostgresqlIndexScans) init() {
	m.data.SetName("postgresql.index.scans")
	m.data.SetDescription("The number of index scans on a table.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexScans) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("postgresql.index.name", postgresqlIndexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexScans) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexScans) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexScans(cfg MetricConfig) metricPostgresqlIndexScans {
	m := metricPostgresqlIndexScans{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlIndexSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.index.size metric with initial data.
func (m *metricPostgresqlIndexSize) init() {
	m.data.SetName("postgresql.index.size")
	m.data.SetDescription("The size of the index on disk.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlIndexSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("postgresql.index.name", postgresqlIndexNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlIndexSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlIndexSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlIndexSize(cfg MetricConfig) metricPostgresqlIndexSize {
	m := metricPostgresqlIndexSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlOperations struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.operations metric with initial data.
func (m *metricPostgresqlOperations) init() {
	m.data.SetName("postgresql.operations")
	m.data.SetDescription("The number of database operations.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlOperations) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, operationAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("operation", operationAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlOperations) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlOperations) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlOperations(cfg MetricConfig) metricPostgresqlOperations {
	m := metricPostgresqlOperations{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryAvgDiskReads struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.avg_disk_reads metric with initial data.
func (m *metricPostgresqlQueryAvgDiskReads) init() {
	m.data.SetName("postgresql.query.avg_disk_reads")
	m.data.SetDescription("Average number of disk reads per query execution.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryAvgDiskReads) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryAvgDiskReads) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryAvgDiskReads) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryAvgDiskReads(cfg MetricConfig) metricPostgresqlQueryAvgDiskReads {
	m := metricPostgresqlQueryAvgDiskReads{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryAvgDiskWrites struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.avg_disk_writes metric with initial data.
func (m *metricPostgresqlQueryAvgDiskWrites) init() {
	m.data.SetName("postgresql.query.avg_disk_writes")
	m.data.SetDescription("Average number of disk writes per query execution.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryAvgDiskWrites) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryAvgDiskWrites) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryAvgDiskWrites) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryAvgDiskWrites(cfg MetricConfig) metricPostgresqlQueryAvgDiskWrites {
	m := metricPostgresqlQueryAvgDiskWrites{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryAvgElapsedTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.avg_elapsed_time metric with initial data.
func (m *metricPostgresqlQueryAvgElapsedTime) init() {
	m.data.SetName("postgresql.query.avg_elapsed_time")
	m.data.SetDescription("Average execution time for the query in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryAvgElapsedTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryAvgElapsedTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryAvgElapsedTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryAvgElapsedTime(cfg MetricConfig) metricPostgresqlQueryAvgElapsedTime {
	m := metricPostgresqlQueryAvgElapsedTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryCPUTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.cpu_time metric with initial data.
func (m *metricPostgresqlQueryCPUTime) init() {
	m.data.SetName("postgresql.query.cpu_time")
	m.data.SetDescription("CPU time consumed by the query in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryCPUTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryCPUTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryCPUTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryCPUTime(cfg MetricConfig) metricPostgresqlQueryCPUTime {
	m := metricPostgresqlQueryCPUTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlQueryExecutionCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.query.execution.count metric with initial data.
func (m *metricPostgresqlQueryExecutionCount) init() {
	m.data.SetName("postgresql.query.execution.count")
	m.data.SetDescription("Number of times the query was executed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlQueryExecutionCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.statement.type", postgresqlStatementTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlQueryExecutionCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlQueryExecutionCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlQueryExecutionCount(cfg MetricConfig) metricPostgresqlQueryExecutionCount {
	m := metricPostgresqlQueryExecutionCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlReplicationDataDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.replication.data_delay metric with initial data.
func (m *metricPostgresqlReplicationDataDelay) init() {
	m.data.SetName("postgresql.replication.data_delay")
	m.data.SetDescription("The amount of data delayed in replication.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlReplicationDataDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, replicationClientAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("replication_client", replicationClientAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlReplicationDataDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlReplicationDataDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlReplicationDataDelay(cfg MetricConfig) metricPostgresqlReplicationDataDelay {
	m := metricPostgresqlReplicationDataDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRollbacks struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rollbacks metric with initial data.
func (m *metricPostgresqlRollbacks) init() {
	m.data.SetName("postgresql.rollbacks")
	m.data.SetDescription("The number of rollbacks.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRollbacks) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRollbacks) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRollbacks) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRollbacks(cfg MetricConfig) metricPostgresqlRollbacks {
	m := metricPostgresqlRollbacks{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlRows struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.rows metric with initial data.
func (m *metricPostgresqlRows) init() {
	m.data.SetName("postgresql.rows")
	m.data.SetDescription("The number of rows.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlRows) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, stateAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
	dp.Attributes().PutStr("state", stateAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlRows) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlRows) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlRows(cfg MetricConfig) metricPostgresqlRows {
	m := metricPostgresqlRows{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlSequentialScans struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.sequential_scans metric with initial data.
func (m *metricPostgresqlSequentialScans) init() {
	m.data.SetName("postgresql.sequential_scans")
	m.data.SetDescription("Number of sequential scans initiated on this table.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlSequentialScans) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlSequentialScans) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlSequentialScans) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlSequentialScans(cfg MetricConfig) metricPostgresqlSequentialScans {
	m := metricPostgresqlSequentialScans{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.count metric with initial data.
func (m *metricPostgresqlTableCount) init() {
	m.data.SetName("postgresql.table.count")
	m.data.SetDescription("Number of user tables in a database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTableCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableCount(cfg MetricConfig) metricPostgresqlTableCount {
	m := metricPostgresqlTableCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableScans struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.scans metric with initial data.
func (m *metricPostgresqlTableScans) init() {
	m.data.SetName("postgresql.table.scans")
	m.data.SetDescription("The number of sequential scans on a table.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTableScans) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableScans) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableScans) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableScans(cfg MetricConfig) metricPostgresqlTableScans {
	m := metricPostgresqlTableScans{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableSize struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.size metric with initial data.
func (m *metricPostgresqlTableSize) init() {
	m.data.SetName("postgresql.table.size")
	m.data.SetDescription("Disk space used by a table.")
	m.data.SetUnit("By")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(false)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityUnspecified)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTableSize) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableSize) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableSize) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableSize(cfg MetricConfig) metricPostgresqlTableSize {
	m := metricPostgresqlTableSize{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTableVacuumCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.table.vacuum.count metric with initial data.
func (m *metricPostgresqlTableVacuumCount) init() {
	m.data.SetName("postgresql.table.vacuum.count")
	m.data.SetDescription("Number of times a table has been manually vacuumed.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTableVacuumCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.schema.name", postgresqlSchemaNameAttributeValue)
	dp.Attributes().PutStr("postgresql.table.name", postgresqlTableNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTableVacuumCount) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTableVacuumCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTableVacuumCount(cfg MetricConfig) metricPostgresqlTableVacuumCount {
	m := metricPostgresqlTableVacuumCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTempFiles struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.temp_files metric with initial data.
func (m *metricPostgresqlTempFiles) init() {
	m.data.SetName("postgresql.temp_files")
	m.data.SetDescription("Number of temporary files created by queries in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTempFiles) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTempFiles) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTempFiles) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTempFiles(cfg MetricConfig) metricPostgresqlTempFiles {
	m := metricPostgresqlTempFiles{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTupDeleted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.tup_deleted metric with initial data.
func (m *metricPostgresqlTupDeleted) init() {
	m.data.SetName("postgresql.tup_deleted")
	m.data.SetDescription("Number of rows deleted by queries in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTupDeleted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTupDeleted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTupDeleted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTupDeleted(cfg MetricConfig) metricPostgresqlTupDeleted {
	m := metricPostgresqlTupDeleted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTupFetched struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.tup_fetched metric with initial data.
func (m *metricPostgresqlTupFetched) init() {
	m.data.SetName("postgresql.tup_fetched")
	m.data.SetDescription("Number of rows fetched by queries in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTupFetched) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTupFetched) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTupFetched) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTupFetched(cfg MetricConfig) metricPostgresqlTupFetched {
	m := metricPostgresqlTupFetched{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTupInserted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.tup_inserted metric with initial data.
func (m *metricPostgresqlTupInserted) init() {
	m.data.SetName("postgresql.tup_inserted")
	m.data.SetDescription("Number of rows inserted by queries in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTupInserted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTupInserted) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTupInserted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTupInserted(cfg MetricConfig) metricPostgresqlTupInserted {
	m := metricPostgresqlTupInserted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTupReturned struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.tup_returned metric with initial data.
func (m *metricPostgresqlTupReturned) init() {
	m.data.SetName("postgresql.tup_returned")
	m.data.SetDescription("Number of rows returned by queries in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTupReturned) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTupReturned) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTupReturned) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTupReturned(cfg MetricConfig) metricPostgresqlTupReturned {
	m := metricPostgresqlTupReturned{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlTupUpdated struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.tup_updated metric with initial data.
func (m *metricPostgresqlTupUpdated) init() {
	m.data.SetName("postgresql.tup_updated")
	m.data.SetDescription("Number of rows updated by queries in this database.")
	m.data.SetUnit("1")
	m.data.SetEmptySum()
	m.data.Sum().SetIsMonotonic(true)
	m.data.Sum().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
	m.data.Sum().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlTupUpdated) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Sum().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlTupUpdated) updateCapacity() {
	if m.data.Sum().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Sum().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlTupUpdated) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Sum().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlTupUpdated(cfg MetricConfig) metricPostgresqlTupUpdated {
	m := metricPostgresqlTupUpdated{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWaitEventTotalTime struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wait.event.total_time metric with initial data.
func (m *metricPostgresqlWaitEventTotalTime) init() {
	m.data.SetName("postgresql.wait.event.total_time")
	m.data.SetDescription("Total wait time for the wait event in milliseconds.")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWaitEventTotalTime) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlWaitEventNameAttributeValue string, postgresqlWaitCategoryAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("postgresql.database.name", postgresqlDatabaseNameAttributeValue)
	dp.Attributes().PutStr("postgresql.query.id", postgresqlQueryIDAttributeValue)
	dp.Attributes().PutStr("postgresql.query.text", postgresqlQueryTextAttributeValue)
	dp.Attributes().PutStr("postgresql.wait.event.name", postgresqlWaitEventNameAttributeValue)
	dp.Attributes().PutStr("postgresql.wait.category", postgresqlWaitCategoryAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWaitEventTotalTime) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWaitEventTotalTime) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWaitEventTotalTime(cfg MetricConfig) metricPostgresqlWaitEventTotalTime {
	m := metricPostgresqlWaitEventTotalTime{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalAge struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.age metric with initial data.
func (m *metricPostgresqlWalAge) init() {
	m.data.SetName("postgresql.wal.age")
	m.data.SetDescription("Age of the oldest WAL file.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
}

func (m *metricPostgresqlWalAge) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalAge) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalAge) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalAge(cfg MetricConfig) metricPostgresqlWalAge {
	m := metricPostgresqlWalAge{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalDelay struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.delay metric with initial data.
func (m *metricPostgresqlWalDelay) init() {
	m.data.SetName("postgresql.wal.delay")
	m.data.SetDescription("Time between flushing recent WAL locally and receiving notification that the standby server has completed an operation with it.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricPostgresqlWalDelay) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, replicationClientAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("replication_client", replicationClientAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalDelay) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalDelay) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalDelay(cfg MetricConfig) metricPostgresqlWalDelay {
	m := metricPostgresqlWalDelay{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricPostgresqlWalLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills postgresql.wal.lag metric with initial data.
func (m *metricPostgresqlWalLag) init() {
	m.data.SetName("postgresql.wal.lag")
	m.data.SetDescription("Time between flushing recent WAL locally and receiving notification that the standby server has completed an operation with it.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
}

func (m *metricPostgresqlWalLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricPostgresqlWalLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricPostgresqlWalLag) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricPostgresqlWalLag(cfg MetricConfig) metricPostgresqlWalLag {
	m := metricPostgresqlWalLag{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config                                           MetricsBuilderConfig // config of the metrics builder.
	startTime                                        pcommon.Timestamp    // start time that will be applied to all recorded data points.
	metricsCapacity                                  int                  // maximum observed number of metrics per resource.
	metricsBuffer                                    pmetric.Metrics      // accumulates metrics data before emitting.
	buildInfo                                        component.BuildInfo  // contains version information.
	resourceAttributeIncludeFilter                   map[string]filter.Filter
	resourceAttributeExcludeFilter                   map[string]filter.Filter
	metricPostgresqlBackends                         metricPostgresqlBackends
	metricPostgresqlBgwriterBuffersAllocated         metricPostgresqlBgwriterBuffersAllocated
	metricPostgresqlBgwriterBuffersWrites            metricPostgresqlBgwriterBuffersWrites
	metricPostgresqlBgwriterCheckpointCount          metricPostgresqlBgwriterCheckpointCount
	metricPostgresqlBgwriterDuration                 metricPostgresqlBgwriterDuration
	metricPostgresqlBgwriterMaxwritten               metricPostgresqlBgwriterMaxwritten
	metricPostgresqlBlksHit                          metricPostgresqlBlksHit
	metricPostgresqlBlksRead                         metricPostgresqlBlksRead
	metricPostgresqlBlockedSessionPid                metricPostgresqlBlockedSessionPid
	metricPostgresqlBlockingSessionDuration          metricPostgresqlBlockingSessionDuration
	metricPostgresqlBlockingSessionPid               metricPostgresqlBlockingSessionPid
	metricPostgresqlBlockingSessionWaitEvent         metricPostgresqlBlockingSessionWaitEvent
	metricPostgresqlBlockingSessionWaitEventType     metricPostgresqlBlockingSessionWaitEventType
	metricPostgresqlBlocksHit                        metricPostgresqlBlocksHit
	metricPostgresqlBlocksRead                       metricPostgresqlBlocksRead
	metricPostgresqlCommits                          metricPostgresqlCommits
	metricPostgresqlConnectionCount                  metricPostgresqlConnectionCount
	metricPostgresqlConnectionMax                    metricPostgresqlConnectionMax
	metricPostgresqlDatabaseCount                    metricPostgresqlDatabaseCount
	metricPostgresqlDatabaseLocks                    metricPostgresqlDatabaseLocks
	metricPostgresqlDbSize                           metricPostgresqlDbSize
	metricPostgresqlDeadlocks                        metricPostgresqlDeadlocks
	metricPostgresqlExecutionPlanActualLoops         metricPostgresqlExecutionPlanActualLoops
	metricPostgresqlExecutionPlanActualRows          metricPostgresqlExecutionPlanActualRows
	metricPostgresqlExecutionPlanActualTotalTime     metricPostgresqlExecutionPlanActualTotalTime
	metricPostgresqlExecutionPlanAsyncCapable        metricPostgresqlExecutionPlanAsyncCapable
	metricPostgresqlExecutionPlanCostActual          metricPostgresqlExecutionPlanCostActual
	metricPostgresqlExecutionPlanCostEstimate        metricPostgresqlExecutionPlanCostEstimate
	metricPostgresqlExecutionPlanIoReadTime          metricPostgresqlExecutionPlanIoReadTime
	metricPostgresqlExecutionPlanIoWriteTime         metricPostgresqlExecutionPlanIoWriteTime
	metricPostgresqlExecutionPlanParallelAware       metricPostgresqlExecutionPlanParallelAware
	metricPostgresqlExecutionPlanPlanRows            metricPostgresqlExecutionPlanPlanRows
	metricPostgresqlExecutionPlanPlanWidth           metricPostgresqlExecutionPlanPlanWidth
	metricPostgresqlExecutionPlanSharedHitBlocks     metricPostgresqlExecutionPlanSharedHitBlocks
	metricPostgresqlExecutionPlanSharedReadBlocks    metricPostgresqlExecutionPlanSharedReadBlocks
	metricPostgresqlExecutionPlanSharedWrittenBlocks metricPostgresqlExecutionPlanSharedWrittenBlocks
	metricPostgresqlExecutionPlanStartupTime         metricPostgresqlExecutionPlanStartupTime
	metricPostgresqlExecutionPlanTempReadBlocks      metricPostgresqlExecutionPlanTempReadBlocks
	metricPostgresqlExecutionPlanTempWrittenBlocks   metricPostgresqlExecutionPlanTempWrittenBlocks
	metricPostgresqlIndexScans                       metricPostgresqlIndexScans
	metricPostgresqlIndexSize                        metricPostgresqlIndexSize
	metricPostgresqlOperations                       metricPostgresqlOperations
	metricPostgresqlQueryAvgDiskReads                metricPostgresqlQueryAvgDiskReads
	metricPostgresqlQueryAvgDiskWrites               metricPostgresqlQueryAvgDiskWrites
	metricPostgresqlQueryAvgElapsedTime              metricPostgresqlQueryAvgElapsedTime
	metricPostgresqlQueryCPUTime                     metricPostgresqlQueryCPUTime
	metricPostgresqlQueryExecutionCount              metricPostgresqlQueryExecutionCount
	metricPostgresqlReplicationDataDelay             metricPostgresqlReplicationDataDelay
	metricPostgresqlRollbacks                        metricPostgresqlRollbacks
	metricPostgresqlRows                             metricPostgresqlRows
	metricPostgresqlSequentialScans                  metricPostgresqlSequentialScans
	metricPostgresqlTableCount                       metricPostgresqlTableCount
	metricPostgresqlTableScans                       metricPostgresqlTableScans
	metricPostgresqlTableSize                        metricPostgresqlTableSize
	metricPostgresqlTableVacuumCount                 metricPostgresqlTableVacuumCount
	metricPostgresqlTempFiles                        metricPostgresqlTempFiles
	metricPostgresqlTupDeleted                       metricPostgresqlTupDeleted
	metricPostgresqlTupFetched                       metricPostgresqlTupFetched
	metricPostgresqlTupInserted                      metricPostgresqlTupInserted
	metricPostgresqlTupReturned                      metricPostgresqlTupReturned
	metricPostgresqlTupUpdated                       metricPostgresqlTupUpdated
	metricPostgresqlWaitEventTotalTime               metricPostgresqlWaitEventTotalTime
	metricPostgresqlWalAge                           metricPostgresqlWalAge
	metricPostgresqlWalDelay                         metricPostgresqlWalDelay
	metricPostgresqlWalLag                           metricPostgresqlWalLag
}

// MetricBuilderOption applies changes to default metrics builder.
type MetricBuilderOption interface {
	apply(*MetricsBuilder)
}

type metricBuilderOptionFunc func(mb *MetricsBuilder)

func (mbof metricBuilderOptionFunc) apply(mb *MetricsBuilder) {
	mbof(mb)
}

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) MetricBuilderOption {
	return metricBuilderOptionFunc(func(mb *MetricsBuilder) {
		mb.startTime = startTime
	})
}
func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.Settings, options ...MetricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:                                           mbc,
		startTime:                                        pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                    pmetric.NewMetrics(),
		buildInfo:                                        settings.BuildInfo,
		metricPostgresqlBackends:                         newMetricPostgresqlBackends(mbc.Metrics.PostgresqlBackends),
		metricPostgresqlBgwriterBuffersAllocated:         newMetricPostgresqlBgwriterBuffersAllocated(mbc.Metrics.PostgresqlBgwriterBuffersAllocated),
		metricPostgresqlBgwriterBuffersWrites:            newMetricPostgresqlBgwriterBuffersWrites(mbc.Metrics.PostgresqlBgwriterBuffersWrites),
		metricPostgresqlBgwriterCheckpointCount:          newMetricPostgresqlBgwriterCheckpointCount(mbc.Metrics.PostgresqlBgwriterCheckpointCount),
		metricPostgresqlBgwriterDuration:                 newMetricPostgresqlBgwriterDuration(mbc.Metrics.PostgresqlBgwriterDuration),
		metricPostgresqlBgwriterMaxwritten:               newMetricPostgresqlBgwriterMaxwritten(mbc.Metrics.PostgresqlBgwriterMaxwritten),
		metricPostgresqlBlksHit:                          newMetricPostgresqlBlksHit(mbc.Metrics.PostgresqlBlksHit),
		metricPostgresqlBlksRead:                         newMetricPostgresqlBlksRead(mbc.Metrics.PostgresqlBlksRead),
		metricPostgresqlBlockedSessionPid:                newMetricPostgresqlBlockedSessionPid(mbc.Metrics.PostgresqlBlockedSessionPid),
		metricPostgresqlBlockingSessionDuration:          newMetricPostgresqlBlockingSessionDuration(mbc.Metrics.PostgresqlBlockingSessionDuration),
		metricPostgresqlBlockingSessionPid:               newMetricPostgresqlBlockingSessionPid(mbc.Metrics.PostgresqlBlockingSessionPid),
		metricPostgresqlBlockingSessionWaitEvent:         newMetricPostgresqlBlockingSessionWaitEvent(mbc.Metrics.PostgresqlBlockingSessionWaitEvent),
		metricPostgresqlBlockingSessionWaitEventType:     newMetricPostgresqlBlockingSessionWaitEventType(mbc.Metrics.PostgresqlBlockingSessionWaitEventType),
		metricPostgresqlBlocksHit:                        newMetricPostgresqlBlocksHit(mbc.Metrics.PostgresqlBlocksHit),
		metricPostgresqlBlocksRead:                       newMetricPostgresqlBlocksRead(mbc.Metrics.PostgresqlBlocksRead),
		metricPostgresqlCommits:                          newMetricPostgresqlCommits(mbc.Metrics.PostgresqlCommits),
		metricPostgresqlConnectionCount:                  newMetricPostgresqlConnectionCount(mbc.Metrics.PostgresqlConnectionCount),
		metricPostgresqlConnectionMax:                    newMetricPostgresqlConnectionMax(mbc.Metrics.PostgresqlConnectionMax),
		metricPostgresqlDatabaseCount:                    newMetricPostgresqlDatabaseCount(mbc.Metrics.PostgresqlDatabaseCount),
		metricPostgresqlDatabaseLocks:                    newMetricPostgresqlDatabaseLocks(mbc.Metrics.PostgresqlDatabaseLocks),
		metricPostgresqlDbSize:                           newMetricPostgresqlDbSize(mbc.Metrics.PostgresqlDbSize),
		metricPostgresqlDeadlocks:                        newMetricPostgresqlDeadlocks(mbc.Metrics.PostgresqlDeadlocks),
		metricPostgresqlExecutionPlanActualLoops:         newMetricPostgresqlExecutionPlanActualLoops(mbc.Metrics.PostgresqlExecutionPlanActualLoops),
		metricPostgresqlExecutionPlanActualRows:          newMetricPostgresqlExecutionPlanActualRows(mbc.Metrics.PostgresqlExecutionPlanActualRows),
		metricPostgresqlExecutionPlanActualTotalTime:     newMetricPostgresqlExecutionPlanActualTotalTime(mbc.Metrics.PostgresqlExecutionPlanActualTotalTime),
		metricPostgresqlExecutionPlanAsyncCapable:        newMetricPostgresqlExecutionPlanAsyncCapable(mbc.Metrics.PostgresqlExecutionPlanAsyncCapable),
		metricPostgresqlExecutionPlanCostActual:          newMetricPostgresqlExecutionPlanCostActual(mbc.Metrics.PostgresqlExecutionPlanCostActual),
		metricPostgresqlExecutionPlanCostEstimate:        newMetricPostgresqlExecutionPlanCostEstimate(mbc.Metrics.PostgresqlExecutionPlanCostEstimate),
		metricPostgresqlExecutionPlanIoReadTime:          newMetricPostgresqlExecutionPlanIoReadTime(mbc.Metrics.PostgresqlExecutionPlanIoReadTime),
		metricPostgresqlExecutionPlanIoWriteTime:         newMetricPostgresqlExecutionPlanIoWriteTime(mbc.Metrics.PostgresqlExecutionPlanIoWriteTime),
		metricPostgresqlExecutionPlanParallelAware:       newMetricPostgresqlExecutionPlanParallelAware(mbc.Metrics.PostgresqlExecutionPlanParallelAware),
		metricPostgresqlExecutionPlanPlanRows:            newMetricPostgresqlExecutionPlanPlanRows(mbc.Metrics.PostgresqlExecutionPlanPlanRows),
		metricPostgresqlExecutionPlanPlanWidth:           newMetricPostgresqlExecutionPlanPlanWidth(mbc.Metrics.PostgresqlExecutionPlanPlanWidth),
		metricPostgresqlExecutionPlanSharedHitBlocks:     newMetricPostgresqlExecutionPlanSharedHitBlocks(mbc.Metrics.PostgresqlExecutionPlanSharedHitBlocks),
		metricPostgresqlExecutionPlanSharedReadBlocks:    newMetricPostgresqlExecutionPlanSharedReadBlocks(mbc.Metrics.PostgresqlExecutionPlanSharedReadBlocks),
		metricPostgresqlExecutionPlanSharedWrittenBlocks: newMetricPostgresqlExecutionPlanSharedWrittenBlocks(mbc.Metrics.PostgresqlExecutionPlanSharedWrittenBlocks),
		metricPostgresqlExecutionPlanStartupTime:         newMetricPostgresqlExecutionPlanStartupTime(mbc.Metrics.PostgresqlExecutionPlanStartupTime),
		metricPostgresqlExecutionPlanTempReadBlocks:      newMetricPostgresqlExecutionPlanTempReadBlocks(mbc.Metrics.PostgresqlExecutionPlanTempReadBlocks),
		metricPostgresqlExecutionPlanTempWrittenBlocks:   newMetricPostgresqlExecutionPlanTempWrittenBlocks(mbc.Metrics.PostgresqlExecutionPlanTempWrittenBlocks),
		metricPostgresqlIndexScans:                       newMetricPostgresqlIndexScans(mbc.Metrics.PostgresqlIndexScans),
		metricPostgresqlIndexSize:                        newMetricPostgresqlIndexSize(mbc.Metrics.PostgresqlIndexSize),
		metricPostgresqlOperations:                       newMetricPostgresqlOperations(mbc.Metrics.PostgresqlOperations),
		metricPostgresqlQueryAvgDiskReads:                newMetricPostgresqlQueryAvgDiskReads(mbc.Metrics.PostgresqlQueryAvgDiskReads),
		metricPostgresqlQueryAvgDiskWrites:               newMetricPostgresqlQueryAvgDiskWrites(mbc.Metrics.PostgresqlQueryAvgDiskWrites),
		metricPostgresqlQueryAvgElapsedTime:              newMetricPostgresqlQueryAvgElapsedTime(mbc.Metrics.PostgresqlQueryAvgElapsedTime),
		metricPostgresqlQueryCPUTime:                     newMetricPostgresqlQueryCPUTime(mbc.Metrics.PostgresqlQueryCPUTime),
		metricPostgresqlQueryExecutionCount:              newMetricPostgresqlQueryExecutionCount(mbc.Metrics.PostgresqlQueryExecutionCount),
		metricPostgresqlReplicationDataDelay:             newMetricPostgresqlReplicationDataDelay(mbc.Metrics.PostgresqlReplicationDataDelay),
		metricPostgresqlRollbacks:                        newMetricPostgresqlRollbacks(mbc.Metrics.PostgresqlRollbacks),
		metricPostgresqlRows:                             newMetricPostgresqlRows(mbc.Metrics.PostgresqlRows),
		metricPostgresqlSequentialScans:                  newMetricPostgresqlSequentialScans(mbc.Metrics.PostgresqlSequentialScans),
		metricPostgresqlTableCount:                       newMetricPostgresqlTableCount(mbc.Metrics.PostgresqlTableCount),
		metricPostgresqlTableScans:                       newMetricPostgresqlTableScans(mbc.Metrics.PostgresqlTableScans),
		metricPostgresqlTableSize:                        newMetricPostgresqlTableSize(mbc.Metrics.PostgresqlTableSize),
		metricPostgresqlTableVacuumCount:                 newMetricPostgresqlTableVacuumCount(mbc.Metrics.PostgresqlTableVacuumCount),
		metricPostgresqlTempFiles:                        newMetricPostgresqlTempFiles(mbc.Metrics.PostgresqlTempFiles),
		metricPostgresqlTupDeleted:                       newMetricPostgresqlTupDeleted(mbc.Metrics.PostgresqlTupDeleted),
		metricPostgresqlTupFetched:                       newMetricPostgresqlTupFetched(mbc.Metrics.PostgresqlTupFetched),
		metricPostgresqlTupInserted:                      newMetricPostgresqlTupInserted(mbc.Metrics.PostgresqlTupInserted),
		metricPostgresqlTupReturned:                      newMetricPostgresqlTupReturned(mbc.Metrics.PostgresqlTupReturned),
		metricPostgresqlTupUpdated:                       newMetricPostgresqlTupUpdated(mbc.Metrics.PostgresqlTupUpdated),
		metricPostgresqlWaitEventTotalTime:               newMetricPostgresqlWaitEventTotalTime(mbc.Metrics.PostgresqlWaitEventTotalTime),
		metricPostgresqlWalAge:                           newMetricPostgresqlWalAge(mbc.Metrics.PostgresqlWalAge),
		metricPostgresqlWalDelay:                         newMetricPostgresqlWalDelay(mbc.Metrics.PostgresqlWalDelay),
		metricPostgresqlWalLag:                           newMetricPostgresqlWalLag(mbc.Metrics.PostgresqlWalLag),
		resourceAttributeIncludeFilter:                   make(map[string]filter.Filter),
		resourceAttributeExcludeFilter:                   make(map[string]filter.Filter),
	}
	if mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.database.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.database.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlDatabaseName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlIndexName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.index.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlIndexName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlIndexName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.index.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlIndexName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlSchemaName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.schema.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlSchemaName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlSchemaName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.schema.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlSchemaName.MetricsExclude)
	}
	if mbc.ResourceAttributes.PostgresqlTableName.MetricsInclude != nil {
		mb.resourceAttributeIncludeFilter["postgresql.table.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlTableName.MetricsInclude)
	}
	if mbc.ResourceAttributes.PostgresqlTableName.MetricsExclude != nil {
		mb.resourceAttributeExcludeFilter["postgresql.table.name"] = filter.CreateFilter(mbc.ResourceAttributes.PostgresqlTableName.MetricsExclude)
	}

	for _, op := range options {
		op.apply(mb)
	}
	return mb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption interface {
	apply(pmetric.ResourceMetrics)
}

type resourceMetricsOptionFunc func(pmetric.ResourceMetrics)

func (rmof resourceMetricsOptionFunc) apply(rm pmetric.ResourceMetrics) {
	rmof(rm)
}

// WithResource sets the provided resource on the emitted ResourceMetrics.
// It's recommended to use ResourceBuilder to create the resource.
func WithResource(res pcommon.Resource) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		res.CopyTo(rm.Resource())
	})
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return resourceMetricsOptionFunc(func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	})
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(options ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName(ScopeName)
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricPostgresqlBackends.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersAllocated.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterBuffersWrites.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterCheckpointCount.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterDuration.emit(ils.Metrics())
	mb.metricPostgresqlBgwriterMaxwritten.emit(ils.Metrics())
	mb.metricPostgresqlBlksHit.emit(ils.Metrics())
	mb.metricPostgresqlBlksRead.emit(ils.Metrics())
	mb.metricPostgresqlBlockedSessionPid.emit(ils.Metrics())
	mb.metricPostgresqlBlockingSessionDuration.emit(ils.Metrics())
	mb.metricPostgresqlBlockingSessionPid.emit(ils.Metrics())
	mb.metricPostgresqlBlockingSessionWaitEvent.emit(ils.Metrics())
	mb.metricPostgresqlBlockingSessionWaitEventType.emit(ils.Metrics())
	mb.metricPostgresqlBlocksHit.emit(ils.Metrics())
	mb.metricPostgresqlBlocksRead.emit(ils.Metrics())
	mb.metricPostgresqlCommits.emit(ils.Metrics())
	mb.metricPostgresqlConnectionCount.emit(ils.Metrics())
	mb.metricPostgresqlConnectionMax.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseCount.emit(ils.Metrics())
	mb.metricPostgresqlDatabaseLocks.emit(ils.Metrics())
	mb.metricPostgresqlDbSize.emit(ils.Metrics())
	mb.metricPostgresqlDeadlocks.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanActualLoops.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanActualRows.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanActualTotalTime.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanAsyncCapable.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanCostActual.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanCostEstimate.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanIoReadTime.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanIoWriteTime.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanParallelAware.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanPlanRows.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanPlanWidth.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanSharedHitBlocks.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanSharedReadBlocks.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanSharedWrittenBlocks.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanStartupTime.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanTempReadBlocks.emit(ils.Metrics())
	mb.metricPostgresqlExecutionPlanTempWrittenBlocks.emit(ils.Metrics())
	mb.metricPostgresqlIndexScans.emit(ils.Metrics())
	mb.metricPostgresqlIndexSize.emit(ils.Metrics())
	mb.metricPostgresqlOperations.emit(ils.Metrics())
	mb.metricPostgresqlQueryAvgDiskReads.emit(ils.Metrics())
	mb.metricPostgresqlQueryAvgDiskWrites.emit(ils.Metrics())
	mb.metricPostgresqlQueryAvgElapsedTime.emit(ils.Metrics())
	mb.metricPostgresqlQueryCPUTime.emit(ils.Metrics())
	mb.metricPostgresqlQueryExecutionCount.emit(ils.Metrics())
	mb.metricPostgresqlReplicationDataDelay.emit(ils.Metrics())
	mb.metricPostgresqlRollbacks.emit(ils.Metrics())
	mb.metricPostgresqlRows.emit(ils.Metrics())
	mb.metricPostgresqlSequentialScans.emit(ils.Metrics())
	mb.metricPostgresqlTableCount.emit(ils.Metrics())
	mb.metricPostgresqlTableScans.emit(ils.Metrics())
	mb.metricPostgresqlTableSize.emit(ils.Metrics())
	mb.metricPostgresqlTableVacuumCount.emit(ils.Metrics())
	mb.metricPostgresqlTempFiles.emit(ils.Metrics())
	mb.metricPostgresqlTupDeleted.emit(ils.Metrics())
	mb.metricPostgresqlTupFetched.emit(ils.Metrics())
	mb.metricPostgresqlTupInserted.emit(ils.Metrics())
	mb.metricPostgresqlTupReturned.emit(ils.Metrics())
	mb.metricPostgresqlTupUpdated.emit(ils.Metrics())
	mb.metricPostgresqlWaitEventTotalTime.emit(ils.Metrics())
	mb.metricPostgresqlWalAge.emit(ils.Metrics())
	mb.metricPostgresqlWalDelay.emit(ils.Metrics())
	mb.metricPostgresqlWalLag.emit(ils.Metrics())

	for _, op := range options {
		op.apply(rm)
	}
	for attr, filter := range mb.resourceAttributeIncludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && !filter.Matches(val.AsString()) {
			return
		}
	}
	for attr, filter := range mb.resourceAttributeExcludeFilter {
		if val, ok := rm.Resource().Attributes().Get(attr); ok && filter.Matches(val.AsString()) {
			return
		}
	}

	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(options ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(options...)
	metrics := mb.metricsBuffer
	mb.metricsBuffer = pmetric.NewMetrics()
	return metrics
}

// RecordPostgresqlBackendsDataPoint adds a data point to postgresql.backends metric.
func (mb *MetricsBuilder) RecordPostgresqlBackendsDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlBackends.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlBgwriterBuffersAllocatedDataPoint adds a data point to postgresql.bgwriter.buffers.allocated metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersAllocatedDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlBgwriterBuffersAllocated.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlBgwriterBuffersWritesDataPoint adds a data point to postgresql.bgwriter.buffers.writes metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterBuffersWritesDataPoint(ts pcommon.Timestamp, val int64, bgBufferSourceAttributeValue AttributeBgBufferSource) {
	mb.metricPostgresqlBgwriterBuffersWrites.recordDataPoint(mb.startTime, ts, val, bgBufferSourceAttributeValue.String())
}

// RecordPostgresqlBgwriterCheckpointCountDataPoint adds a data point to postgresql.bgwriter.checkpoint.count metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterCheckpointCountDataPoint(ts pcommon.Timestamp, val int64, bgCheckpointTypeAttributeValue AttributeBgCheckpointType) {
	mb.metricPostgresqlBgwriterCheckpointCount.recordDataPoint(mb.startTime, ts, val, bgCheckpointTypeAttributeValue.String())
}

// RecordPostgresqlBgwriterDurationDataPoint adds a data point to postgresql.bgwriter.duration metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterDurationDataPoint(ts pcommon.Timestamp, val float64, bgDurationTypeAttributeValue AttributeBgDurationType) {
	mb.metricPostgresqlBgwriterDuration.recordDataPoint(mb.startTime, ts, val, bgDurationTypeAttributeValue.String())
}

// RecordPostgresqlBgwriterMaxwrittenDataPoint adds a data point to postgresql.bgwriter.maxwritten metric.
func (mb *MetricsBuilder) RecordPostgresqlBgwriterMaxwrittenDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlBgwriterMaxwritten.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlBlksHitDataPoint adds a data point to postgresql.blks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlBlksHitDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlBlksHit.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlBlksReadDataPoint adds a data point to postgresql.blks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlBlksReadDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlBlksRead.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlBlockedSessionPidDataPoint adds a data point to postgresql.blocked.session.pid metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockedSessionPidDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	mb.metricPostgresqlBlockedSessionPid.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryTextAttributeValue)
}

// RecordPostgresqlBlockingSessionDurationDataPoint adds a data point to postgresql.blocking.session.duration metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockingSessionDurationDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlBlockedQueryTextAttributeValue string, postgresqlBlockingQueryTextAttributeValue string) {
	mb.metricPostgresqlBlockingSessionDuration.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlBlockedQueryTextAttributeValue, postgresqlBlockingQueryTextAttributeValue)
}

// RecordPostgresqlBlockingSessionPidDataPoint adds a data point to postgresql.blocking.session.pid metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockingSessionPidDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryTextAttributeValue string) {
	mb.metricPostgresqlBlockingSessionPid.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryTextAttributeValue)
}

// RecordPostgresqlBlockingSessionWaitEventDataPoint adds a data point to postgresql.blocking.session.wait_event metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockingSessionWaitEventDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlBlockedQueryTextAttributeValue string, postgresqlBlockingQueryTextAttributeValue string, postgresqlWaitEventAttributeValue string) {
	mb.metricPostgresqlBlockingSessionWaitEvent.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlBlockedQueryTextAttributeValue, postgresqlBlockingQueryTextAttributeValue, postgresqlWaitEventAttributeValue)
}

// RecordPostgresqlBlockingSessionWaitEventTypeDataPoint adds a data point to postgresql.blocking.session.wait_event_type metric.
func (mb *MetricsBuilder) RecordPostgresqlBlockingSessionWaitEventTypeDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlBlockedQueryTextAttributeValue string, postgresqlBlockingQueryTextAttributeValue string, postgresqlWaitEventTypeAttributeValue string) {
	mb.metricPostgresqlBlockingSessionWaitEventType.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlBlockedQueryTextAttributeValue, postgresqlBlockingQueryTextAttributeValue, postgresqlWaitEventTypeAttributeValue)
}

// RecordPostgresqlBlocksHitDataPoint adds a data point to postgresql.blocks_hit metric.
func (mb *MetricsBuilder) RecordPostgresqlBlocksHitDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue AttributeSource) {
	mb.metricPostgresqlBlocksHit.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, sourceAttributeValue.String())
}

// RecordPostgresqlBlocksReadDataPoint adds a data point to postgresql.blocks_read metric.
func (mb *MetricsBuilder) RecordPostgresqlBlocksReadDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, sourceAttributeValue AttributeSource) {
	mb.metricPostgresqlBlocksRead.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, sourceAttributeValue.String())
}

// RecordPostgresqlCommitsDataPoint adds a data point to postgresql.commits metric.
func (mb *MetricsBuilder) RecordPostgresqlCommitsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlCommits.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlConnectionCountDataPoint adds a data point to postgresql.connection.count metric.
func (mb *MetricsBuilder) RecordPostgresqlConnectionCountDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlConnectionCount.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlConnectionMaxDataPoint adds a data point to postgresql.connection.max metric.
func (mb *MetricsBuilder) RecordPostgresqlConnectionMaxDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlConnectionMax.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlDatabaseCountDataPoint adds a data point to postgresql.database.count metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseCountDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlDatabaseCount.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlDatabaseLocksDataPoint adds a data point to postgresql.database.locks metric.
func (mb *MetricsBuilder) RecordPostgresqlDatabaseLocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlDatabaseLocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlDbSizeDataPoint adds a data point to postgresql.db_size metric.
func (mb *MetricsBuilder) RecordPostgresqlDbSizeDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlDbSize.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlDeadlocksDataPoint adds a data point to postgresql.deadlocks metric.
func (mb *MetricsBuilder) RecordPostgresqlDeadlocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlDeadlocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlExecutionPlanActualLoopsDataPoint adds a data point to postgresql.execution_plan.actual_loops metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanActualLoopsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanActualLoops.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanActualRowsDataPoint adds a data point to postgresql.execution_plan.actual_rows metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanActualRowsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanActualRows.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanActualTotalTimeDataPoint adds a data point to postgresql.execution_plan.actual_total_time metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanActualTotalTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanActualTotalTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanAsyncCapableDataPoint adds a data point to postgresql.execution_plan.async_capable metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanAsyncCapableDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanAsyncCapable.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanCostActualDataPoint adds a data point to postgresql.execution_plan.cost_actual metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanCostActualDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanCostActual.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanCostEstimateDataPoint adds a data point to postgresql.execution_plan.cost_estimate metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanCostEstimateDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanCostEstimate.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanIoReadTimeDataPoint adds a data point to postgresql.execution_plan.io_read_time metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanIoReadTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanIoReadTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanIoWriteTimeDataPoint adds a data point to postgresql.execution_plan.io_write_time metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanIoWriteTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanIoWriteTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanParallelAwareDataPoint adds a data point to postgresql.execution_plan.parallel_aware metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanParallelAwareDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanParallelAware.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanPlanRowsDataPoint adds a data point to postgresql.execution_plan.plan_rows metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanPlanRowsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanPlanRows.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanPlanWidthDataPoint adds a data point to postgresql.execution_plan.plan_width metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanPlanWidthDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanPlanWidth.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanSharedHitBlocksDataPoint adds a data point to postgresql.execution_plan.shared_hit_blocks metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanSharedHitBlocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanSharedHitBlocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanSharedReadBlocksDataPoint adds a data point to postgresql.execution_plan.shared_read_blocks metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanSharedReadBlocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanSharedReadBlocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanSharedWrittenBlocksDataPoint adds a data point to postgresql.execution_plan.shared_written_blocks metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanSharedWrittenBlocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanSharedWrittenBlocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanStartupTimeDataPoint adds a data point to postgresql.execution_plan.startup_time metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanStartupTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanStartupTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanTempReadBlocksDataPoint adds a data point to postgresql.execution_plan.temp_read_blocks metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanTempReadBlocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanTempReadBlocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlExecutionPlanTempWrittenBlocksDataPoint adds a data point to postgresql.execution_plan.temp_written_blocks metric.
func (mb *MetricsBuilder) RecordPostgresqlExecutionPlanTempWrittenBlocksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlNodeTypeAttributeValue string) {
	mb.metricPostgresqlExecutionPlanTempWrittenBlocks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlNodeTypeAttributeValue)
}

// RecordPostgresqlIndexScansDataPoint adds a data point to postgresql.index.scans metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexScansDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	mb.metricPostgresqlIndexScans.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue, postgresqlIndexNameAttributeValue)
}

// RecordPostgresqlIndexSizeDataPoint adds a data point to postgresql.index.size metric.
func (mb *MetricsBuilder) RecordPostgresqlIndexSizeDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string, postgresqlIndexNameAttributeValue string) {
	mb.metricPostgresqlIndexSize.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue, postgresqlIndexNameAttributeValue)
}

// RecordPostgresqlOperationsDataPoint adds a data point to postgresql.operations metric.
func (mb *MetricsBuilder) RecordPostgresqlOperationsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, operationAttributeValue AttributeOperation) {
	mb.metricPostgresqlOperations.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, operationAttributeValue.String())
}

// RecordPostgresqlQueryAvgDiskReadsDataPoint adds a data point to postgresql.query.avg_disk_reads metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryAvgDiskReadsDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryAvgDiskReads.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlQueryAvgDiskWritesDataPoint adds a data point to postgresql.query.avg_disk_writes metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryAvgDiskWritesDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryAvgDiskWrites.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlQueryAvgElapsedTimeDataPoint adds a data point to postgresql.query.avg_elapsed_time metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryAvgElapsedTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryAvgElapsedTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlQueryCPUTimeDataPoint adds a data point to postgresql.query.cpu_time metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryCPUTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string) {
	mb.metricPostgresqlQueryCPUTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue)
}

// RecordPostgresqlQueryExecutionCountDataPoint adds a data point to postgresql.query.execution.count metric.
func (mb *MetricsBuilder) RecordPostgresqlQueryExecutionCountDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlStatementTypeAttributeValue string) {
	mb.metricPostgresqlQueryExecutionCount.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlStatementTypeAttributeValue)
}

// RecordPostgresqlReplicationDataDelayDataPoint adds a data point to postgresql.replication.data_delay metric.
func (mb *MetricsBuilder) RecordPostgresqlReplicationDataDelayDataPoint(ts pcommon.Timestamp, val int64, replicationClientAttributeValue string) {
	mb.metricPostgresqlReplicationDataDelay.recordDataPoint(mb.startTime, ts, val, replicationClientAttributeValue)
}

// RecordPostgresqlRollbacksDataPoint adds a data point to postgresql.rollbacks metric.
func (mb *MetricsBuilder) RecordPostgresqlRollbacksDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlRollbacks.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlRowsDataPoint adds a data point to postgresql.rows metric.
func (mb *MetricsBuilder) RecordPostgresqlRowsDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlTableNameAttributeValue string, stateAttributeValue AttributeState) {
	mb.metricPostgresqlRows.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlTableNameAttributeValue, stateAttributeValue.String())
}

// RecordPostgresqlSequentialScansDataPoint adds a data point to postgresql.sequential_scans metric.
func (mb *MetricsBuilder) RecordPostgresqlSequentialScansDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	mb.metricPostgresqlSequentialScans.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue)
}

// RecordPostgresqlTableCountDataPoint adds a data point to postgresql.table.count metric.
func (mb *MetricsBuilder) RecordPostgresqlTableCountDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlTableCount.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlTableScansDataPoint adds a data point to postgresql.table.scans metric.
func (mb *MetricsBuilder) RecordPostgresqlTableScansDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	mb.metricPostgresqlTableScans.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue)
}

// RecordPostgresqlTableSizeDataPoint adds a data point to postgresql.table.size metric.
func (mb *MetricsBuilder) RecordPostgresqlTableSizeDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	mb.metricPostgresqlTableSize.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue)
}

// RecordPostgresqlTableVacuumCountDataPoint adds a data point to postgresql.table.vacuum.count metric.
func (mb *MetricsBuilder) RecordPostgresqlTableVacuumCountDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string, postgresqlSchemaNameAttributeValue string, postgresqlTableNameAttributeValue string) {
	mb.metricPostgresqlTableVacuumCount.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlSchemaNameAttributeValue, postgresqlTableNameAttributeValue)
}

// RecordPostgresqlTempFilesDataPoint adds a data point to postgresql.temp_files metric.
func (mb *MetricsBuilder) RecordPostgresqlTempFilesDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlTempFiles.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlTupDeletedDataPoint adds a data point to postgresql.tup_deleted metric.
func (mb *MetricsBuilder) RecordPostgresqlTupDeletedDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlTupDeleted.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlTupFetchedDataPoint adds a data point to postgresql.tup_fetched metric.
func (mb *MetricsBuilder) RecordPostgresqlTupFetchedDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlTupFetched.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlTupInsertedDataPoint adds a data point to postgresql.tup_inserted metric.
func (mb *MetricsBuilder) RecordPostgresqlTupInsertedDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlTupInserted.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlTupReturnedDataPoint adds a data point to postgresql.tup_returned metric.
func (mb *MetricsBuilder) RecordPostgresqlTupReturnedDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlTupReturned.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlTupUpdatedDataPoint adds a data point to postgresql.tup_updated metric.
func (mb *MetricsBuilder) RecordPostgresqlTupUpdatedDataPoint(ts pcommon.Timestamp, val int64, postgresqlDatabaseNameAttributeValue string) {
	mb.metricPostgresqlTupUpdated.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue)
}

// RecordPostgresqlWaitEventTotalTimeDataPoint adds a data point to postgresql.wait.event.total_time metric.
func (mb *MetricsBuilder) RecordPostgresqlWaitEventTotalTimeDataPoint(ts pcommon.Timestamp, val float64, postgresqlDatabaseNameAttributeValue string, postgresqlQueryIDAttributeValue string, postgresqlQueryTextAttributeValue string, postgresqlWaitEventNameAttributeValue string, postgresqlWaitCategoryAttributeValue string) {
	mb.metricPostgresqlWaitEventTotalTime.recordDataPoint(mb.startTime, ts, val, postgresqlDatabaseNameAttributeValue, postgresqlQueryIDAttributeValue, postgresqlQueryTextAttributeValue, postgresqlWaitEventNameAttributeValue, postgresqlWaitCategoryAttributeValue)
}

// RecordPostgresqlWalAgeDataPoint adds a data point to postgresql.wal.age metric.
func (mb *MetricsBuilder) RecordPostgresqlWalAgeDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricPostgresqlWalAge.recordDataPoint(mb.startTime, ts, val)
}

// RecordPostgresqlWalDelayDataPoint adds a data point to postgresql.wal.delay metric.
func (mb *MetricsBuilder) RecordPostgresqlWalDelayDataPoint(ts pcommon.Timestamp, val float64, replicationClientAttributeValue string) {
	mb.metricPostgresqlWalDelay.recordDataPoint(mb.startTime, ts, val, replicationClientAttributeValue)
}

// RecordPostgresqlWalLagDataPoint adds a data point to postgresql.wal.lag metric.
func (mb *MetricsBuilder) RecordPostgresqlWalLagDataPoint(ts pcommon.Timestamp, val float64) {
	mb.metricPostgresqlWalLag.recordDataPoint(mb.startTime, ts, val)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...MetricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op.apply(mb)
	}
}
